[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Data Modelling",
    "section": "",
    "text": "Welcome\nThis is the website for the course: “Spatial Data Modelling for Population Science using R”. The course was designed and is delivered by Professor Francisco Rowe from the Geographic Data Science Lab at the Department of Geography and Planning from the University of Liverpool, United Kingdom.\n\n\n\n\n\n\nNote\n\n\n\nA PDF version of this course is available for download here.\n\n\n\n\nContact\n\n\n\n\nFrancisco Rowe - fcorowe [at] liverpool.ac.uk - Professor in Population Data Science - Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, UK.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01a_overview.html",
    "href": "01a_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Aim\nThis course provides an introduction to Spatial Data Modelling for Population Science using R. It has been designed to be delivered in 15 hours of teaching, split into five sessions of 3 hours each, including breaks, practical examples and questions. This course provides an intuitive and practical explanation of methods, modelling frameworks and theories for analysing spatial data, as well as demographic and social processes represented in space. Spatial data have become ubiquitous. Data containing location have become increasingly common in both volume and frequency with the development and use of new sensor, satellite technilogy and GPS-based technologies, such as smartphones, social media platforms and contactless payment systems. At the same time, social and demographic processes occur within a geographic context that conditions or triggers their evolution. Therefore, these processes must be studied explicitly, considering the spatial context in which they take place. For these reasons, techniques that enable the analysis and modelling of spatial data are essential to understand demographic and social processes to inform policy and business decision-making.\nThe objective of the course is to expose students to a variety of techniques for the analysis and modelling of spatial data. The course discusses and illustrates the types of spatial data and the challenges they present, introducing a range of modelling and analytical approaches that explicitly account for space. It also aims to develop programming skills in the R language. These skills are important for several reasons: programming is in high demand, it teaches logical and methodical thinking, and it allows ideas to be materialised into tangible products such as maps. The course also aims to introduce generative artificial intelligence through the use of ChatGPT for code writing, debugging and reasoning.",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "01a_overview.html#learning-outcomes",
    "href": "01a_overview.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the module, students should be able to:\n\nidentify some key sources of spatial data and resources of spatial analysis and modelling tools;\n\nexplain the advantages of taking spatial structure into account when analysing spatial data;\n\napply a range of computer-based techniques for the analysis of spatial data, including mapping, correlation, kernel density estimation, spatial interaction modelling and spatial econometrics;\n\napply appropriate analytical strategies to tackle the key methodological challenges facing spatial analysis – spatial autocorrelation, heterogeneity, and ecological fallacy; and,\n\nselect appropriate analytical tools for analysing specific spatial data sets to address emerging social issues facing the society.",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "01a_overview.html#teaching-approach",
    "href": "01a_overview.html#teaching-approach",
    "title": "Overview",
    "section": "Teaching Approach",
    "text": "Teaching Approach\nThe course combines lectures with a strong practical component and independent work. Each session will cover one of the following components:\n\nTypes of spatial data.\n\nManipulation and visualisation of spatial data.\n\nAnalysis of location data.\n\nAnalysis of spatial relationship data.\n\nModels that capture spatial correlation.\n\n\nThe course: - Introduces these techniques in an intuitive and applied manner rather than through theoretical mathematical or statistical derivations.\n\n\nFocuses on explaining when and why a technique is appropriate, followed by its application.\n\nUses several practical examples based on real data and research questions.\n\nIs built around the R programming language and uses structured computational notebooks compiled in a book, including code with explanations that can be reused with other datasets.",
    "crumbs": [
      "Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "01b_environment.html",
    "href": "01b_environment.html",
    "title": "Environment",
    "section": "",
    "text": "To reproduce the code in the book, you need the following software packages:\n\nR version 4.5.1 (2025-06-13)\n\nRStudio Version 2025.09.1+401 (2025.09.1+401)\nQuarto 1.8.25\nthe list of libraries in the next section\n\nTo check your version of:\n\nR and libraries run sessionInfo()\nRStudio click help on the menu bar and then About\nQuarto check the version file in the quarto folder on your computer.\n\nTo install and update:\n\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN)\nRStudio, download the appropriate version from Posit\nQuarto, download the appropriate version from the Quarto website\n\n\nDependency list\nThe list of libraries used in this book is provided below:\n\narm\ncar\ncorrplot\ndevtools\nFRK\ngghighlight\nggplot2\nggmap\nGISTools\ngridExtra\ngstat\nggthemes\nhexbin\njtools\nkableExtra\nknitr\nleaflet\nlme4\nlmtest\nlubridate\nmapdeck\nMASS\nmerTools\nplyr\npatchwork\nRColorBrewer\nrgdal\nscales\nsf\nsjPlot\nsp\nspgwr\nshowtext\nspatialreg\nspacetime\nstargazer\ntidyverse\ntmap\ntufte\nviridis\nbasemapR\n\nCopy, paste and run the code below in your console. Ensure all packages are installed on your computer.\n\n# package names\npackages &lt;- c(\n    \"arm\",\n    \"car\",\n    \"corrplot\",\n    \"devtools\",\n    \"FRK\",\n    \"gghighlight\",\n    \"ggplot2\",\n    \"ggmap\",\n    \"gridExtra\",\n    \"gstat\",\n    \"hexbin\",\n    \"jtools\",\n    \"kableExtra\",\n    \"knitr\",\n    \"lme4\",\n    \"lmtest\",\n    \"lubridate\",\n    \"MASS\",\n    \"merTools\",\n    \"plyr\",\n    \"RColorBrewer\",\n    \"sf\",\n    \"sjPlot\",\n    \"sp\",\n    \"spgwr\",\n    \"spatialreg\",\n    \"spacetime\",\n    \"stargazer\",\n    \"tidyverse\",\n    \"tmap\",\n    \"tufte\",\n    \"viridis\"\n)\n\n# install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# packages loading\ninvisible(lapply(packages, library, character.only = TRUE))\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo install the library basemapR, you need to install from source by running:\nlibrary(devtools) install_github('Chrisjb/basemapR')",
    "crumbs": [
      "Introduction",
      "Environment"
    ]
  },
  {
    "objectID": "01c_assessment.html",
    "href": "01c_assessment.html",
    "title": "Assessment",
    "section": "",
    "text": "Assignment 1: Spatial Data Infographic (40%)\nThe course assessment will consist of two components.\nDescription\nThe first assignment involves the creation of an infographic that visually communicates a spatial data story. The goal is to synthesise spatial data, analytical results and visual design into a clear, evidence-based narrative that can fit within a single PowerPoint slide. The infographic must demonstrate understanding of spatial concepts, analytical methods and effective data communication principles.\nInstructions\nEvaluation Criteria (100 points total)\nPurpose\nThis assignment develops skills in communicating spatial evidence succinctly and visually. This is an essential competence for academic, policy and professional audiences.",
    "crumbs": [
      "Introduction",
      "Assessment"
    ]
  },
  {
    "objectID": "01c_assessment.html#assignment-1-spatial-data-infographic-40",
    "href": "01c_assessment.html#assignment-1-spatial-data-infographic-40",
    "title": "Assessment",
    "section": "",
    "text": "Topic – Select a spatially explicit social, demographic or environmental issue (e.g., population change, internal migration, population decline, population ageing).\nData – Use the geospatial datasets on internal migration or population described in the Chapter dataset, or a comparable open dataset. Clearly cite all sources.\nContent – Present a concise spatial narrative integrating:\n\nA key research question or problem.\nRelevant spatial analysis or modelling results (maps, graphs and / or summary statistics).\nInterpretation of spatial patterns and implications.\nReferences or data sources.\n\nFormat –\n\nOne PowerPoint slide (16:9 ratio).\nAll text must be legible when projected on screen.\nInclude a clear title and author name.\nVisuals may include maps, charts, icons or minimal text boxes.\nUse consistent colour-blind friendly palette and typographic hierarchy.\n\nSubmission – Upload the PowerPoint (.pptx) file and a PDF version.\n\n\n\n\n\n\n\n\n\n\nCriterion\nDescription\nWeight\n\n\n\n\nRelevance and clarity of question\nDefines a meaningful spatial issue and communicates the core message clearly.\n20 %\n\n\nAnalytical rigor\nDemonstrates appropriate use of spatial data and analysis; accuracy of methods and results.\n25 %\n\n\nInterpretation\nInterprets spatial patterns logically and links findings to broader demographic or social processes.\n20 %\n\n\nDesign and visual communication\nEffective visual hierarchy, readability, layout, and use of colour; slide visually balanced and professional.\n25 %\n\n\nData and source transparency\nCorrect data citation and ethical use of sources.\n10 %",
    "crumbs": [
      "Introduction",
      "Assessment"
    ]
  },
  {
    "objectID": "01c_assessment.html#assignment-2-computational-essay-60",
    "href": "01c_assessment.html#assignment-2-computational-essay-60",
    "title": "Assessment",
    "section": "Assignment 2: Computational Essay (60%)",
    "text": "Assignment 2: Computational Essay (60%)\nDescription\nThe second assignment consists of a computational essay that integrates narrative, code and visualisation to explore a spatially explicit research question. A computational essay is a document in which the story, analysis and computation are woven together: The text explains the logic of the analysis, the code performs it, and the figures or tables present the results. This format reflects the way spatial data scientists communicate reproducible, evidence-based insights.\nObjectives\nThe computational essay should:\n\nDemonstrate the ability to apply spatial analysis or modelling techniques learned in the course.\nIntegrate theory, data and computation into a coherent narrative.\nPresent clear, reproducible and interpretable results using R.\nCommunicate findings effectively through maps, graphics and concise text.\n\n\nInstructions\n\nTopic Selection\nChoose a spatial problem relevant to the social, demographic, or environmental domains (e.g., population change, internal migration, population decline, population ageing). You may use the geospatial datasets on internal migration or population described in the Chapter dataset, or a comparable open dataset.\nStructure\n\nIntroduction (≈300–400 words): State the research question and explain its spatial relevance.\nData and Methods (≈400–600 words): Describe the data sources, spatial structure and analytical or modelling techniques used. Include annotated R code chunks performing the analysis.\nResults and Discussion (≈600–800 words): Present and interpret results using visualisations and tables. Discuss implications, limitations and potential extensions.\nConclusion (≈200–300 words): Summarise key findings and their significance.\nReferences (not included in the word count): Include all data and bibliographic sources using a consistent citation style.\n\nFormat and Submission\n\nMaximum 2,000 words (excluding code, references and captions).\nInclude R code directly within the essay in Quarto format.\nInclude up to five figures (a figure may contain multiple maps or plots if they form a single integrated result).\nInclude up to one table.\nSubmit both the rendered PDF or HTML output and the corresponding .qmd source file. Ensure the active the option self-contained in the YALM of the .qmd file. This ensures all figures are embedded within the final HTML file.\nAll analyses must be fully reproducible using the submitted code and data. Share data via a link to the platform where they are stored such as Google Drive, Dropbox or GitHub.\n\n\n\n\nEvaluation Criteria (100 points total)\n\n\n\n\n\n\n\n\nCriterion\nDescription\nWeight\n\n\n\n\nRelevance and research question\nClearly defined, spatially meaningful question grounded in theory or application.\n15%\n\n\nAnalytical rigor and reproducibility\nCorrect and appropriate use of spatial data and methods; code runs and reproduces results.\n25%\n\n\nIntegration of code and narrative\nNarrative, computation, and visuals are well-linked; code is readable and annotated.\n20%\n\n\nInterpretation and critical insight\nResults are interpreted thoughtfully, demonstrating understanding of spatial processes.\n20%\n\n\nPresentation and communication\nClarity of writing, structure, figure quality, and adherence to limits.\n15%\n\n\nTransparency and referencing\nComplete and accurate citation of data and literature sources.\n5%\n\n\n\n\nPurpose\nThis assignment develops core competencies in computational thinking, reproducible spatial analysis, and scientific communication. It trains students to present spatial data analyses as coherent narratives supported by transparent, executable code. This is a skill essential for modern population and spatial data science.",
    "crumbs": [
      "Introduction",
      "Assessment"
    ]
  },
  {
    "objectID": "02a_spatial-data.html",
    "href": "02a_spatial-data.html",
    "title": "1  Spatial Data",
    "section": "",
    "text": "1.1 Spatial Data types\nThis Chapter seeks to present and describe distinctive attributes of spatial data, and discuss some of the main challenges in analysing and modelling these data. Spatial data is a term used to describe any data associating a given variable attribute to a specific location on the Earth’s surface.\nDifferent classifications of spatial data types exist. Knowing the structure of the data at hand is important as specific analytical methods would be more appropriate for particular data types. We will use a particular classification involving four data types: lattice/areal data, point data, flow data and trajectory data (Fig. 1). This is not a exhaustive list but it is helpful to motivate the analytical and modelling methods that we cover in this book.\nLattice/Areal Data. These data correspond to records of attribute values (such as population counts) for a fixed geographical area. They may comprise regular shapes (such as grids or pixels) or irregular shapes (such as states, counties or travel-to-work areas). Raster data are a common source of regular lattice/areal area, while censuses are probably the most common form of irregular lattice/areal area. Point data within an area can be aggregated to produce lattice/areal data.\nPoint Data. These data refer to records of the geographic location of an discrete event, or the number of occurrences of geographical process at a given location. As displayed in Fig. 1, examples include the geographic location of bus stops in a city, or the number of boarding passengers at each bus stop.\nFlow Data. These data refer to records of measurements for a pair of geographic point locations. or pair of areas. These data capture the linkage or spatial interaction between two locations. Migration flows between a place of origin and a place of destination is an example of this type of data.\nTrajectory Data. These data record geographic locations of moving objects at various points in time. A trajectory is composed of a single string of data recording the geographic location of an object at various points in time and each record in the string contains a time stamp. These data are complex and can be classified into explicit trajectory data and implicit trajectory data. The former refer to well-structured data and record positions of objects continuously and intensively at uniform time intervals, such as GPS data. The latter is less structured and record data in relatively time point intervals, including sensor-based, network-based and signal-based data (Kong et al. 2018).\nIn this course, we cover analytical and modelling approaches for point, lattice/areal and flow data. While we do not explicitly analyse trajectory data, various of the analytical approaches described in this book can be extended to incorporate time, and can be applied to model these types of data. Rowe and Arribas-Bel (2022) provide an introduction on approaches to analyse and model spatio-temporal data. These same methods can be applied to trajectory data.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "02a_spatial-data.html#spatial-data-types",
    "href": "02a_spatial-data.html#spatial-data-types",
    "title": "1  Spatial Data",
    "section": "",
    "text": "Fig. 1. Data Types. Area / Lattice data source: Önnerfors et al. (2019). Point data source: Tao et al. (2018). Flow data source: Rowe and Patias (2020). Trajectory data source: Kwan and Lee (2004).",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "02a_spatial-data.html#hierarchical-structure-of-data",
    "href": "02a_spatial-data.html#hierarchical-structure-of-data",
    "title": "1  Spatial Data",
    "section": "1.2 Hierarchical Structure of Data",
    "text": "1.2 Hierarchical Structure of Data\nThe hierarchical organisation is a key feature of spatial data. Smaller geographical units are organised within larger geographical units. You can find the hierarchical representation of UK Statistical Geographies on the Office for National Statistics website. In the bottom part of the output below, we can observe a spatial data frame for Liverpool displaying the hierarchical structure of census data (from the smallest to the largest): Output Areas (OAs), Lower Super Output Areas (LSOAs), Middle Super Output Areas (MSOAs) and Local Authority Districts (LADs). This hierarchical structure entails that units in smaller geographies are nested within units in larger geographies, and that smaller units can be aggregated to produce large units.\n\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 335071.6 ymin: 389876.7 xmax: 339426.9 ymax: 394479\nProjected CRS: Transverse_Mercator\n      OA_CD   LSOA_CD   MSOA_CD    LAD_CD                       geometry\n1 E00176737 E01033761 E02006932 E08000012 MULTIPOLYGON (((335106.3 38...\n2 E00033515 E01006614 E02001358 E08000012 MULTIPOLYGON (((335810.5 39...\n3 E00033141 E01006546 E02001365 E08000012 MULTIPOLYGON (((336738 3931...\n4 E00176757 E01006646 E02001369 E08000012 MULTIPOLYGON (((335914.5 39...\n5 E00034050 E01006712 E02001375 E08000012 MULTIPOLYGON (((339325 3914...\n6 E00034280 E01006761 E02001366 E08000012 MULTIPOLYGON (((338198.1 39...\n\n\nNext we quickly go through the components of the output above. The first line indicates the type of feature and the number of rows (features) and columns (fields) in the data frame, except for the geometry. The second and third lines identify the type of geometry and dimension. The fourth line bbox stands for bounding box and display the min and max coordinates containing the Liverpool area in the data frame. The fifth line projected CRS indicates the coordinate reference system projection. If you would like to learn more about the various components of spatial data frames, please see the R sf package vignette on Simple Features.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "02a_spatial-data.html#key-challenges",
    "href": "02a_spatial-data.html#key-challenges",
    "title": "1  Spatial Data",
    "section": "1.3 Key Challenges",
    "text": "1.3 Key Challenges\nMajor challenges exist when working with spatial data. Below we explore some of the key longstanding problems data scientists often face when working with geographical data.\n\n1.3.1 Modifible Area Unit Problem (MAUP)\nThe Modifible Area Unit Problem (MAUP) represents a challenge that has troubled geographers for decades (Openshaw 1981). Two aspects of the MAUP are normally recognised in empirical analysis relating to scale and zonation. Fig. 2 illustrates these issues\n\nScale refers to the idea that a geographical area can be divided into geographies with differing numbers of spatial units.\nZonation refers to the idea that a geographical area can be divided into the same number of units in a variety of ways.\n\n\n\n\nFig. 2. MAUP effect. (a) scale effect; and, (b) zonation effect. Source: Loidl et al. (2016).\n\n\nThe MAUP is a critical issue as it can impact our analysis and thus any conclusions we can infer from our results (e.g. Fotheringham and Wong 1991). There is no agreed systematic approach on how to handle the effects of the MAUP. Some have suggested to perform analyses based on different existing geographical scales, and assess the consistency of the results and identify potential sources of change. The issue with such approach is that results from analysis at different scales are likely to differ because distinct dimensions of a geographic process may be captured at different scales. For example, in migration studies, smaller geographies may be more suitable to capture residential mobility over short distances, while large geographies may be more suitable to capture long-distance migration. And it is well documented that these types of moves are driven by different factors. While residential mobility tends to be driven by housing related reasons, long-distance migration is more closely related to employment-related motives (Niedomysl 2011).\nAn alternative approach is to use the smallest geographical system available and create random aggregations at various geographical scales, to directly quantify the extent of scale and zonation. This approach has shown promising results in applications to study internal migration flows (Stillwell, Daras, and Bell 2018). Another approach involves the production of “meaningful” or functional geographies that can more appropriately capture the process of interest. There is an active area of work defining functional labour markets (Casado-Díaz, Martínez-Bernabéu, and Rowe 2017), urban areas (Arribas-Bel, Garcia-López, and Viladecans-Marsal 2021) and various forms of geodemographic classifications (Singleton and Spielman 2013; Patias, Rowe, and Cavazzi 2019) . However there is the recognition that none of the existing approaches resolve the effects of the MAUP and recently it has been suggested that the most plausible ‘solution’ would be to ignore the MAUP (Wolf et al. 2020).\n\n\n1.3.2 Ecological Fallacy\nEcological fallacy is an error in the interpretation of statistical data based on aggregate information. Specifically it refers to inferences made about the nature of specific individuals based solely on statistics aggregated for a given group. It is about thinking that relationships observed for groups necessarily hold for individuals. A key example is Robinson (1950) who illustrates this problem exploring the difference between ecological correlations and individual correlations. He looked at the relationship between country of birth and literacy. Robinson (1950) used the percent of foreign-born population and percent of literate population for the 48 states in the United States in 1930. The ecological correlation based on these data was 0.53. This suggests a positive association between foreign birth and literacy, and could be interpreted as foreign born individuals being more likely to be literate than native-born individuals. Yet, the correlation based on individual data was negative -0.11 which indicates the opposite. The main point emerging from this example is to carefully interpret analysis based on spatial data and avoid making inferences about individuals from these data.\n\n\n1.3.3 Spatial Dependence\nSpatial dependence refers to the spatial relationship of a variable’s values for a pair of locations at a certain distance apart, so that these values are more similar (or less similar) than expected for randomly associated pairs of observations (Anselin 1988). For example, we could think of observed patterns of ethnic segregation in an area are a result of spillover effects of pre-existing patterns of ethnic segregation in neighbouring areas. Chapter 5 will illustrate approach to explicitly incorporate spatial dependence in regression analysis.\n\n\n1.3.4 Spatial Heterogeneity\nSpatial heterogeneity refers to the uneven distribution of a variable’s values across space. Concentration of deprivation or unemployment across an area are good examples of spatial heterogeneity. We illustrate various ways to visualise, explore and measure the spatial distribution of data in multiple chapters. We also discuss on how to use spatial econometrics to capture spatial heterogeneity in Chapter 4 and Chapter 5.\n\n\n1.3.5 Spatial nonstationarity\nSpatial nonstationarity refers to variations in the relationship between an outcome variable and a set of predictor variables across space. In a modelling context, it relates to a situation in which a simple “global” model is inappropriate to explain the relationships between a set of variables. The geographical nature of the model must be modified to reflect local structural relationships within the data. For example, ethinic segregation has been positively associated with employment outcomes in some countries pointing to networks in pre-existing communities facilitating access to the local labour market. Inversely ethnic segregation has been negatively associated with employment outcomes pointing to lack of integration into the broader local community. We do not cover modelling approaches for this issue. Interested readers could refer to Rowe and Arribas-Bel (2022).\n\n\n\n\nAnselin, Luc. 1988. Spatial Econometrics: Methods and Models. Vol. 4. Springer Science & Business Media.\n\n\nArribas-Bel, Daniel, M.-À. Garcia-López, and Elisabet Viladecans-Marsal. 2021. “Building(s and) Cities: Delineating Urban Areas with a Machine Learning Algorithm.” Journal of Urban Economics 125 (September): 103217. https://doi.org/10.1016/j.jue.2019.103217.\n\n\nCasado-Díaz, José Manuel, Lucas Martínez-Bernabéu, and Francisco Rowe. 2017. “An Evolutionary Approach to the Delimitation of Labour Market Areas: An Empirical Application for Chile.” Spatial Economic Analysis 12 (4): 379–403. https://doi.org/10.1080/17421772.2017.1273541.\n\n\nFotheringham, A Stewart, and David WS Wong. 1991. “The Modifiable Areal Unit Problem in Multivariate Statistical Analysis.” Environment and Planning A 23 (7): 1025–44.\n\n\nKong, Xiangjie, Menglin Li, Kai Ma, Kaiqi Tian, Mengyuan Wang, Zhaolong Ning, and Feng Xia. 2018. “Big Trajectory Data: A Survey of Applications and Services.” IEEE Access 6: 58295–306. https://doi.org/10.1109/access.2018.2873779.\n\n\nKwan, Mei-Po, and Jiyeong Lee. 2004. “Geovisualization of Human Activity Patterns Using 3D GIS: A Time-Geographic Approach.” Spatially Integrated Social Science 27: 721–44.\n\n\nLoidl, Martin, Gudrun Wallentin, Robin Wendel, and Bernhard Zagel. 2016. “Mapping Bicycle Crash Risk Patterns on the Local Scale.” Safety 2 (3): 17. https://doi.org/10.3390/safety2030017.\n\n\nNiedomysl, Thomas. 2011. “How Migration Motives Change over Migration Distance: Evidence on Variation Across Socio-Economic and Demographic Groups.” Regional Studies 45 (6): 843–55. https://doi.org/10.1080/00343401003614266.\n\n\nÖnnerfors, Åsa, Mariana Kotzeva, Teodóra Brandmüller, et al. 2019. “Eurostat Regional Yearbook 2019 Edition.”\n\n\nOpenshaw, Stan. 1981. “The Modifiable Areal Unit Problem.” Quantitative Geography: A British View, 60–69.\n\n\nPatias, Nikos, Francisco Rowe, and Stefano Cavazzi. 2019. “A Scalable Analytical Framework for Spatio-Temporal Analysis of Neighborhood Change: A Sequence Analysis Approach.” In, 223–41. Springer International Publishing. https://doi.org/10.1007/978-3-030-14745-7_13.\n\n\nRobinson, WS. 1950. “Ecological Correlations and Individual Behavior.” American Sociological Review 15 (195): 351–57.\n\n\nRowe, Francisco, and Dani Arribas-Bel. 2022. “Spatial Modelling for Data Scientists.” University of Liverpool.\n\n\nRowe, Francisco, and Nikos Patias. 2020. “Mapping the Spatial Patterns of Internal Migration in Europe.” Regional Studies, Regional Science 7 (1): 390–93. https://doi.org/10.1080/21681376.2020.1811139.\n\n\nSingleton, Alexander D., and Seth E. Spielman. 2013. “The Past, Present, and Future of Geodemographic Research in the United States and United Kingdom.” The Professional Geographer 66 (4): 558–67. https://doi.org/10.1080/00330124.2013.848764.\n\n\nStillwell, John, Konstantinos Daras, and Martin Bell. 2018. “Spatial Aggregation Methods for Investigating the MAUP Effects in Migration Analysis.” Applied Spatial Analysis and Policy 11 (4): 693–711. https://doi.org/10.1007/s12061-018-9274-6.\n\n\nTao, Sui, Jonathan Corcoran, Francisco Rowe, and Mark Hickman. 2018. “To Travel or Not to Travel: ‘Weather’ Is the Question. Modelling the Effect of Local Weather Conditions on Bus Ridership.” Transportation Research Part C: Emerging Technologies 86 (January): 147–67. https://doi.org/10.1016/j.trc.2017.11.005.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2020. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3): 596–608. https://doi.org/10.1177/0309132520924722.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "02b_geomanipulation.html",
    "href": "02b_geomanipulation.html",
    "title": "2  Spatial data manipulation and visualisation",
    "section": "",
    "text": "2.1 Dependencies\nThis Chapter aims to demonstrate how we can manipulate, wrangle and visualise spatial data in R.\nWe ensure to load the libraries below. A core area of this session is learning to work with spatial data in R. R offers an ecosystem of purposely designed packages for manipulation and visualisation of spatial data and spatial analysis techniques. These ecosystem is known a r-spatial. Various packages exist in CRAN (The Comprehensive R Archive Network), including sf (E. Pebesma 2018, 2022a), stars (E. Pebesma 2022b), terra, s2 (Dunnington, Pebesma, and Rubak 2023), lwgeom (E. Pebesma 2023), gstat (E. J. Pebesma 2004; E. Pebesma and Graeler 2022), spdep (Bivand 2022), spatialreg (Bivand and Piras 2022), spatstat (Baddeley, Rubak, and Turner 2015; Baddeley, Turner, and Rubak 2022), tmap (Tennekes 2018, 2022), mapview (Appelhans et al. 2022) and more. A key package is this ecosystem is sf (E. Pebesma and Bivand 2023). R package sf provides a table format for simple features, where feature geometries are stored in a list-column. It appears in 2016 and was developed to move spatial data analysis in R closer to standards-based approaches seen in the industry and open source projects, to build upon more modern versions of open source geospatial software stack and allow for integration of R spatial software with the tidyverse (Wickham et al. 2019), particularly ggplot2, dplyr, and tidyr. Hence, this course uses sf for data manipulation and analysis.\n# data wrangling\nlibrary(tidyverse)\n\n# spatial data wrangling\nlibrary(sf)\nlibrary(sp) \n\n# data visualisation\nlibrary(viridis) \nlibrary(RColorBrewer) \n\n# format data visualisations\nlibrary(ggthemes)\nlibrary(patchwork)\nlibrary(showtext)\nlibrary(scales)\n\n# create maps\nlibrary(leaflet)\nlibrary(tmap)\nlibrary(mapdeck)",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data manipulation and visualisation</span>"
    ]
  },
  {
    "objectID": "02b_geomanipulation.html#footnotes",
    "href": "02b_geomanipulation.html#footnotes",
    "title": "2  Spatial data manipulation and visualisation",
    "section": "",
    "text": "You can install package mypackage by running the command install.packages(\"mypackage\") on the R prompt or through the Tools --&gt; Install Packages... menu in RStudio.↩︎",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data manipulation and visualisation</span>"
    ]
  },
  {
    "objectID": "02b_geomanipulation.html#dependencies",
    "href": "02b_geomanipulation.html#dependencies",
    "title": "2  Spatial data manipulation and visualisation",
    "section": "",
    "text": "Appelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan Woellauer. 2022. Mapview: Interactive Viewing of Spatial Data in r. https://github.com/r-spatial/mapview.\n\n\nBaddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point Patterns: Methodology and Applications with r. CRC press.\n\n\nBaddeley, Adrian, Rolf Turner, and Ege Rubak. 2022. Spatstat: Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests. http://spatstat.org/.\n\n\nBivand, Roger. 2022. Spdep: Spatial Dependence: Weighting Schemes, Statistics.\n\n\nBivand, Roger, and Gianfranco Piras. 2022. Spatialreg: Spatial Regression Analysis. https://CRAN.R-project.org/package=spatialreg.\n\n\nDunnington, Dewey, Edzer Pebesma, and Ege Rubak. 2023. S2: Spherical Geometry Operators Using the S2 Geometry Library. https://CRAN.R-project.org/package=s2.\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\n———. 2022a. Sf: Simple Features for r. https://CRAN.R-project.org/package=sf.\n\n\n———. 2022b. Stars: Spatiotemporal Arrays, Raster and Vector Data Cubes. https://CRAN.R-project.org/package=stars.\n\n\n———. 2023. Lwgeom: Bindings to Selected Liblwgeom Functions for Simple Features. https://github.com/r-spatial/lwgeom/.\n\n\nPebesma, Edzer J. 2004. “Multivariable Geostatistics in S: The Gstat Package.” Computers & Geosciences 30 (7): 683–91. https://doi.org/10.1016/j.cageo.2004.03.012.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in r. CRC Press.\n\n\nPebesma, Edzer, and Benedikt Graeler. 2022. Gstat: Spatial and Spatio-Temporal Geostatistical Modelling, Prediction and Simulation. https://github.com/r-spatial/gstat/.\n\n\nTennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06.\n\n\n———. 2022. Tmap: Thematic Maps. https://github.com/r-tmap/tmap.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data manipulation and visualisation</span>"
    ]
  },
  {
    "objectID": "02b_geomanipulation.html#data",
    "href": "02b_geomanipulation.html#data",
    "title": "2  Spatial data manipulation and visualisation",
    "section": "2.2 Data",
    "text": "2.2 Data\nHere we read all the data needed for the analysis. We use two types of data: (1) human mobility and population derived from Meta-Facebook users; and, (2) administative boundary data for Chile. The data on Facebook users have been compressed and saved into rds format; hence, a special function (i.e. readRDS ) is needed. The administrative boundaries are available in shp format. To read this file, we will need sf.\n\n2.2.1 Meta-Facebook mobility data\nWe use origin-destination mobility flow data between Provinces in Chile. We use data for April 2020. For a detailed description of the Meta-Facebook mobility data, please see the Chapter Datasets. We start by reading the data. We filter only flows occurring within the boundaries of Chile. The dataset contains daily flow counts between provinces that occurred in April 2020 during three windows of time during the day; that is, between 12am, 8am and 4pm.\nWe have a look at the data frame. We can see that the data contains 20 columns and 29,491 origin-destination interactions capturing counts of movements between provinces.\n\n# read\ndf20 &lt;- readRDS(\"./data/fb/movement_adm/2020_04.rds\") %&gt;% \n  dplyr::filter(country == \"CL\")\nglimpse(df20)\n\nRows: 29,491\nColumns: 20\n$ GEOMETRY                     &lt;chr&gt; \"LINESTRING (-69.96872527689874 -23.40113…\n$ date_time                    &lt;chr&gt; \"2020-04-01 00:00\", \"2020-04-01 00:00\", \"…\n$ start_polygon_id             &lt;chr&gt; \"60845\", \"60862\", \"60845\", \"60862\", \"6086…\n$ start_polygon_name           &lt;chr&gt; \"Antofagasta\", \"Cardenal Caro\", \"Antofaga…\n$ end_polygon_id               &lt;chr&gt; \"60847\", \"60890\", \"60846\", \"60863\", \"6089…\n$ end_polygon_name             &lt;chr&gt; \"Tocopilla\", \"Melipilla\", \"El Loa\", \"Colc…\n$ length_km                    &lt;dbl&gt; 139.7543134, 24.0764953, 100.3339392, 24.…\n$ tile_size                    &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1…\n$ country                      &lt;chr&gt; \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\",…\n$ level                        &lt;chr&gt; \"LEVEL3\", \"LEVEL3\", \"LEVEL3\", \"LEVEL3\", \"…\n$ n_crisis                     &lt;dbl&gt; 79, 18, 320, 71, NA, 369, NA, 20, 11, NA,…\n$ n_baseline                   &lt;dbl&gt; 59.50, 25.50, 671.00, 132.75, NA, 559.25,…\n$ n_difference                 &lt;dbl&gt; 19.50, -7.50, -351.00, -61.75, NA, -190.2…\n$ percent_change               &lt;dbl&gt; 32.231405, -28.301887, -52.232143, -46.16…\n$ is_statistically_significant &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ z_score                      &lt;dbl&gt; 2.0148470, -0.6837043, -4.0000000, -1.261…\n$ start_lat                    &lt;dbl&gt; -24.32798, -34.32667, -24.32798, -34.3266…\n$ start_lon                    &lt;dbl&gt; -69.56718, -71.78028, -69.56718, -71.7802…\n$ end_lat                      &lt;dbl&gt; -22.06894, -33.75413, -22.81611, -34.7036…\n$ end_lon                      &lt;dbl&gt; -69.60081, -71.19829, -68.20015, -71.0631…\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe the column GEOMETRY is what makes a spatial data special. Ensure you understand the information it contains.\n\n\nWe can identify the list of origin and destination provinces for which we can observe movement.\n\nunique_origins &lt;- unique(df20$start_polygon_name)\nunique_destinations &lt;- unique(df20$end_polygon_name)\n\n\n\n2.2.2 Meta-Facebook active users population\nWe will also use information on the number of Meta-Facebook active users population. The population Meta-Facebook active users can vary over time reflecting their varying patterns of usage and internet accessibility.\n\n# read and select observations\npop20_df &lt;- readRDS(\"./data/fb/population_adm/2020_04.rds\") %&gt;% \n  dplyr::filter(country == \"CL\")\n\n# identify polygons\nunique_areas &lt;- unique(pop20_df$polygon_name)\n\n# data overview\nglimpse(pop20_df)\n\nRows: 4,950\nColumns: 14\n$ spaco_id         &lt;dbl&gt; 60870, 60859, 60893, 60862, 60872, 60850, 60869, 6086…\n$ country          &lt;chr&gt; \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\",…\n$ polygon_name     &lt;chr&gt; \"Bío-Bío\", \"San Antonio\", \"Ranco\", \"Cardenal Caro\", \"…\n$ level            &lt;chr&gt; \"LEVEL3\", \"LEVEL3\", \"LEVEL3\", \"LEVEL3\", \"LEVEL3\", \"LE…\n$ date_time        &lt;chr&gt; \"2020-04-01 00:00\", \"2020-04-01 00:00\", \"2020-04-01 0…\n$ n_baseline       &lt;dbl&gt; 35459.9167, 25048.9167, 8326.1780, 6164.6667, 68088.1…\n$ n_crisis         &lt;dbl&gt; 34606, 18033, 7336, 4034, 60602, 6632, 11397, 11397, …\n$ density_baseline &lt;dbl&gt; 0.014799224, 0.010454185, 0.003474937, 0.002572828, 0…\n$ density_crisis   &lt;dbl&gt; 0.014445053, 0.007527239, 0.003062154, 0.001683851, 0…\n$ n_difference     &lt;dbl&gt; -853.91667, -7015.91667, -990.17805, -2130.66667, -74…\n$ percent_change   &lt;dbl&gt; -2.40805018, -28.00774454, -11.89091959, -34.55695518…\n$ clipped_z_score  &lt;dbl&gt; -0.70353211, -1.45851614, -1.20047618, -1.29612464, -…\n$ lat              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ lon              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n2.2.3 Administrative areas\nWe now read the boundaries for Chilean provinces. Provinces are the second administrative level in the country. Provinces are amalgamations of municipalities or comunes, and groupings of provinces are known as regions. Chile is organised around 15 regions, 54 provinces and 346 municipalities - see here.\nLet’s stop here and understand the spatial data frame or sf object we are reading. We can see it has 56 features (i.e. rows) and 5 fields (columns) within a bounding box which defines the area we can visualise on a map. You can see how the map of provinces below seems off to the right. That is because the bounding box has been set to include Chilean islands off of the coast of the country on the Pacific ocean. We will work on adjusting this at a later point in this session.\nThe line CRS or Coordinate Reference Systems identifies the projection system currently attached to the data. This would be the CRS that will be used if we decided to map the data. The component is incredible important if you intend to combine information from two spatial data frames. Ensure they are on the same CRS! A good idea is to used planar projection systems. Lovelace, Nowosad, and Muenchow (2019) provide a good discussion on the various projection systems.\n\nshp_pro &lt;- read_sf(\"./data/shp/adm/province/PROVINCIAS_2020.shp\") %&gt;% \n  st_simplify(preserveTopology =T,\n              dTolerance = 1000) %&gt;%  # 1km\n  sf::st_make_valid() \nshp_pro\n\nSimple feature collection with 56 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.4488 ymin: -55.98085 xmax: -66.42812 ymax: -17.4984\nGeodetic CRS:  GCS_SIRGAS-Chile\n# A tibble: 56 × 6\n   CUT_REG CUT_PROV REGION        PROVINCIA SUPERFICIE                  geometry\n * &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 08      081      Biobío        Concepci…      3469. (((-72.77092 -37.002, -7…\n 2 01      014      Tarapacá      Tamarugal     39428. (((-68.53576 -21.06468, …\n 3 04      042      Coquimbo      Choapa        10131. (((-70.55491 -31.50954, …\n 4 05      054      Valparaíso    Petorca        4589. (((-71.46609 -32.49606, …\n 5 10      101      Los Lagos     Llanquih…     14857. (((-73.06627 -41.89869, …\n 6 11      114      Aysén del Ge… General …     11758. (((-71.75146 -46.33864, …\n 7 05      057      Valparaíso    San Feli…      2636. (((-70.61116 -32.81214, …\n 8 12      121      Magallanes y… Magallan…     37193. (((-73.03711 -54.47267, …\n 9 13      136      Metropolitan… Talagante       581. (((-70.79251 -33.69482, …\n10 15      152      Arica y Pari… Parinaco…      8142. (((-68.91415 -18.89034, …\n# ℹ 46 more rows\n\n\n\n\n\n\n\n\n\n\n\nWe will also use the regional boundaries for visualisation purposes. For now, we will just read them.\n\nshp_reg &lt;- read_sf(\"./data/shp/adm/region/REGIONES_2020.shp\") %&gt;% \n  st_simplify(preserveTopology =T,\n              dTolerance = 1000) %&gt;%  # 1km\n  sf::st_make_valid()",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data manipulation and visualisation</span>"
    ]
  },
  {
    "objectID": "02b_geomanipulation.html#spatial-data-manipulation",
    "href": "02b_geomanipulation.html#spatial-data-manipulation",
    "title": "2  Spatial data manipulation and visualisation",
    "section": "2.3 Spatial data manipulation",
    "text": "2.3 Spatial data manipulation\nNow we will learn how to manipulate spatial data frames in R. We will start with some commonly used basic functions before transitioning to manipulating our data frames to build mobility indicators for data visualisation.\n\n2.3.1 Basic functions\nLet’s start with some basic functions for spatial data handling.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have any questions about what any of these functions does, or how they work. Run, for example:\n?st_read\n\n\nView the structure of the data frame.\n\nglimpse(df20)\n\nRows: 29,491\nColumns: 20\n$ GEOMETRY                     &lt;chr&gt; \"LINESTRING (-69.96872527689874 -23.40113…\n$ date_time                    &lt;chr&gt; \"2020-04-01 00:00\", \"2020-04-01 00:00\", \"…\n$ start_polygon_id             &lt;chr&gt; \"60845\", \"60862\", \"60845\", \"60862\", \"6086…\n$ start_polygon_name           &lt;chr&gt; \"Antofagasta\", \"Cardenal Caro\", \"Antofaga…\n$ end_polygon_id               &lt;chr&gt; \"60847\", \"60890\", \"60846\", \"60863\", \"6089…\n$ end_polygon_name             &lt;chr&gt; \"Tocopilla\", \"Melipilla\", \"El Loa\", \"Colc…\n$ length_km                    &lt;dbl&gt; 139.7543134, 24.0764953, 100.3339392, 24.…\n$ tile_size                    &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1…\n$ country                      &lt;chr&gt; \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\",…\n$ level                        &lt;chr&gt; \"LEVEL3\", \"LEVEL3\", \"LEVEL3\", \"LEVEL3\", \"…\n$ n_crisis                     &lt;dbl&gt; 79, 18, 320, 71, NA, 369, NA, 20, 11, NA,…\n$ n_baseline                   &lt;dbl&gt; 59.50, 25.50, 671.00, 132.75, NA, 559.25,…\n$ n_difference                 &lt;dbl&gt; 19.50, -7.50, -351.00, -61.75, NA, -190.2…\n$ percent_change               &lt;dbl&gt; 32.231405, -28.301887, -52.232143, -46.16…\n$ is_statistically_significant &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ z_score                      &lt;dbl&gt; 2.0148470, -0.6837043, -4.0000000, -1.261…\n$ start_lat                    &lt;dbl&gt; -24.32798, -34.32667, -24.32798, -34.3266…\n$ start_lon                    &lt;dbl&gt; -69.56718, -71.78028, -69.56718, -71.7802…\n$ end_lat                      &lt;dbl&gt; -22.06894, -33.75413, -22.81611, -34.7036…\n$ end_lon                      &lt;dbl&gt; -69.60081, -71.19829, -68.20015, -71.0631…\n\n\nView variable names only.\n\nnames(df20)\n\n [1] \"GEOMETRY\"                     \"date_time\"                   \n [3] \"start_polygon_id\"             \"start_polygon_name\"          \n [5] \"end_polygon_id\"               \"end_polygon_name\"            \n [7] \"length_km\"                    \"tile_size\"                   \n [9] \"country\"                      \"level\"                       \n[11] \"n_crisis\"                     \"n_baseline\"                  \n[13] \"n_difference\"                 \"percent_change\"              \n[15] \"is_statistically_significant\" \"z_score\"                     \n[17] \"start_lat\"                    \"start_lon\"                   \n[19] \"end_lat\"                      \"end_lon\"                     \n\n\nView first few observations.\n\nhead(df20)\n\n# A tibble: 6 × 20\n  GEOMETRY          date_time start_polygon_id start_polygon_name end_polygon_id\n  &lt;chr&gt;             &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;         \n1 LINESTRING (-69.… 2020-04-… 60845            Antofagasta        60847         \n2 LINESTRING (-71.… 2020-04-… 60862            Cardenal Caro      60890         \n3 LINESTRING (-69.… 2020-04-… 60845            Antofagasta        60846         \n4 LINESTRING (-71.… 2020-04-… 60862            Cardenal Caro      60863         \n5 LINESTRING (-70.… 2020-04-… 60860            San Felipe de Aco… 60891         \n6 LINESTRING (-70.… 2020-04-… 60889            Maipo              60891         \n# ℹ 15 more variables: end_polygon_name &lt;chr&gt;, length_km &lt;dbl&gt;,\n#   tile_size &lt;dbl&gt;, country &lt;chr&gt;, level &lt;chr&gt;, n_crisis &lt;dbl&gt;,\n#   n_baseline &lt;dbl&gt;, n_difference &lt;dbl&gt;, percent_change &lt;dbl&gt;,\n#   is_statistically_significant &lt;dbl&gt;, z_score &lt;dbl&gt;, start_lat &lt;dbl&gt;,\n#   start_lon &lt;dbl&gt;, end_lat &lt;dbl&gt;, end_lon &lt;dbl&gt;\n\n\nSelect specific columns, for example, the origin polygon name.\n\ndf20 %&gt;% \n  select(start_polygon_name)\n\n# A tibble: 29,491 × 1\n   start_polygon_name     \n   &lt;chr&gt;                  \n 1 Antofagasta            \n 2 Cardenal Caro          \n 3 Antofagasta            \n 4 Cardenal Caro          \n 5 San Felipe de Aconcagua\n 6 Maipo                  \n 7 Colchagua              \n 8 San Felipe de Aconcagua\n 9 Colchagua              \n10 Antofagasta            \n# ℹ 29,481 more rows\n\n\nFilter specific values from a column, for example, all the flows originating from Antofagasta in the 2020-04-01 00:00 time slot.\n\ndf20 %&gt;% \n  dplyr::select( c(start_polygon_name, end_polygon_name, date_time) ) %&gt;% \n  dplyr::filter(start_polygon_name == \"Antofagasta\" & date_time == \"2020-04-01 00:00\")\n\n# A tibble: 8 × 3\n  start_polygon_name end_polygon_name date_time       \n  &lt;chr&gt;              &lt;chr&gt;            &lt;chr&gt;           \n1 Antofagasta        Tocopilla        2020-04-01 00:00\n2 Antofagasta        El Loa           2020-04-01 00:00\n3 Antofagasta        Concepción       2020-04-01 00:00\n4 Antofagasta        Chañaral         2020-04-01 00:00\n5 Antofagasta        Santiago         2020-04-01 00:00\n6 Antofagasta        Iquique          2020-04-01 00:00\n7 Antofagasta        Antofagasta      2020-04-01 00:00\n8 Antofagasta        Copiapó          2020-04-01 00:00\n\n\nCreate a new column, for example, a column reporting the difference between the n_crisis and n_baseline columns.\n\ndf20 %&gt;% \n  dplyr::select( c(n_baseline, n_crisis) ) %&gt;% \n  mutate( new_column = n_baseline - n_crisis)\n\n# A tibble: 29,491 × 3\n   n_baseline n_crisis new_column\n        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1       59.5       79      -19.5\n 2       25.5       18        7.5\n 3      671        320      351  \n 4      133.        71       61.8\n 5       NA         NA       NA  \n 6      559.       369      190. \n 7       NA         NA       NA  \n 8       42.8       20       22.8\n 9       32.2       11       21.2\n10       NA         NA       NA  \n# ℹ 29,481 more rows\n\n\n\n\n2.3.2 Spatial indicators of human mobility\nWe move to computing spatial indicators to analyse the patterns of human mobility. Normally we use indicators that enable summarising the scale, patterns and changes of mobility. These indices often comprise aggregations of data in meaningful ways, based on geographical or temporal units. Summarising data of human mobility is complex as we are simultenously dealing with three elements: origins, destinations and their interactions. So we often compute indicators to examine these three dimensions.\n\n\n2.3.3 Origin-based indicators\nWe start by computing indicators from the perspective of origin areas. We generate a set of five indicators capturing average and cumulative patterns at the monthly level. We compute the following indicators:\n\nmean_perchange: is the average percent change in the count of movements from a given origin in relation to the baseline period over a month. It is computed as the average of the ratio of the number of movements from an area at time period t, minus the number of movements from an area during the baseline pre-pandemic period over the number of pmovements from an area during the baseline period.\nmean_diff_flow: is the difference in the count of movements from a given origin in relation to the baseline period over a month. It is computed as the difference between the number of movements from an area at time period t, minus the number of movements from an area during the baseline pre-pandemic period.\nsum_diff_flow: is the sum of the difference in the count of movements from a given origin in relation to the baseline period. It is computed as the sum of the difference between the number of movements from an area at time period t, minus the number of movements from an area during the baseline pre-pandemic period.\nmean_outflow: is the average count of movements from a given origin at a give time period over a month.\nsum_outflow: is the sum of the count of movements from a given origin at a give time period over a month.\n\n\norigin_df &lt;- df20 %&gt;% \n  filter(start_polygon_name != end_polygon_name) %&gt;% \n  group_by(start_polygon_name) %&gt;% \n  dplyr::summarise(\n    mean_perchange = mean(percent_change, na.rm = T),\n    mean_diff_flow = mean(n_difference, na.rm = T),\n    sum_diff_flow = sum(n_difference, na.rm = T),\n    mean_outflow = mean(n_crisis, na.rm = T),\n    sum_outflow = sum(n_crisis, na.rm = T)\n    ) %&gt;% \n  ungroup()\n\ntail(origin_df, 10)\n\n# A tibble: 10 × 6\n   start_polygon_name mean_perchange mean_diff_flow sum_diff_flow mean_outflow\n   &lt;chr&gt;                       &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n 1 Santiago                   -70.6         -792.      -1309523.         612. \n 2 Talagante                  -33.8         -263.       -127657.         269. \n 3 Talca                      -52.4         -150.        -70768          115. \n 4 Tamarugal                   24.0          -53.0        -7261.          90.8\n 5 Tierra del Fuego           -76.7          -19.8         -119.          12.2\n 6 Tocopilla                  147.            -4.23        -702.          38.5\n 7 Valdivia                   -75.8         -234.        -65400.          48.3\n 8 Valparaíso                 -62.3         -350.       -257762.         169. \n 9 Ñuble                      -62.5         -165.        -56482.          60.4\n10 Última Esperanza             1.77         -21.4          -21.4         10  \n# ℹ 1 more variable: sum_outflow &lt;dbl&gt;\n\n\nWe observe the observations at the tail of the data frame. We observe that Santiago recorded an average reduction of over 70% in the count of movements from Santiago in April 2020, in relation to the baseline. Yet, the average size of this reduction in outflows seems to have been relatively small, involving 792 and reporting an average outflow of 612 movements.\n\n\n\n\n\n\nQuestion 1\n\n\n\nAnalyse the origin-based indicators and interpret the patterns displayed for Valparaíso.\n\n\n\n\n2.3.4 Destination-based indicators\nMobility involves the spatial interaction between two areas. Mobility impacts both areas of origins and destinations (Rowe, Lovelace, and Dennett 2022). We often seek to understand both perspectives. Here we compute similar indicators to those calculated for the areas of origins. What interesting patterns can you see?\n\n\n\n\n\n\nQuestion 2\n\n\n\nAnalyse the designation-based indicators and interpret the patterns displayed for Santiago.\n\n\n\ndestination_df &lt;- df20 %&gt;% \n  filter(start_polygon_name != end_polygon_name) %&gt;% \n  group_by(end_polygon_name) %&gt;% \n  dplyr::summarise(\n    mean_perchange = mean(percent_change, na.rm = T),\n    mean_diff_flow = mean(n_difference, na.rm = T),\n    sum_diff_flow = sum(n_difference, na.rm = T),\n    mean_inflow = mean(n_crisis, na.rm = T),\n    sum_inflow = sum(n_crisis, na.rm = T)\n    ) %&gt;% \n  ungroup()\n\ntail(destination_df, 10)\n\n# A tibble: 10 × 6\n   end_polygon_name mean_perchange mean_diff_flow sum_diff_flow mean_inflow\n   &lt;chr&gt;                     &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n 1 Santiago                  -72.1       -850.        -1336029.       639. \n 2 Talagante                 -30.3       -266.         -128140        273. \n 3 Talca                     -52.1       -147.          -68827        117. \n 4 Tamarugal                  30.0        -49.4          -7360.        88.9\n 5 Tierra del Fuego          -68.3        -16             -112         13.1\n 6 Tocopilla                  63.3         -0.899         -165.        36.6\n 7 Valdivia                  -71.2       -230.          -63284.        48.2\n 8 Valparaíso                -66.2       -341.         -245913.       170. \n 9 Ñuble                     -63.1       -158.          -57870.        58.0\n10 Última Esperanza          -12.8        NaN                0        NaN  \n# ℹ 1 more variable: sum_inflow &lt;dbl&gt;\n\n\n\n\n2.3.5 Netflows\nAn additional key dimension of mobility is its net balance; that is, the extent to which human mobility acts to redistribute population across areas of the country. While some areas report population gains, other experience decline.\n\n\n\n\n\n\n\n\nNote\n\n\n\nDuring the early stages of COVID-19, photo evidence of empty cities around the world poured over the Internet. Anecdotal evidence suggested the occurrence of an urban exodus, particularly from large cities as people sought more and greener spaces (see some our work on COVID and mobility, e.g. Rowe et al. 2022). At the same time, they sought to avoid high density areas, and they realised the need for bigger housing areas as they become multi-functional spaces for work, study and leisure for entire family units. People were said to have moved to sparsely populated rural areas, suburban spaces and more attractive migration destinations.\n\n\nWe compute a measure of the net balance of mobility flows between areas of origin and destination. We use the average number of movements into an areas, minus the number of movements out of that area. We have a look at the head of the data frame and observe that the average net mobility balance is virtually zero. However, at this point, we should recognise that while indicators are useful to measure different dimensions of human mobility, data visualisations are key to identify systematic patterns in the data.\n\n# mean outflow by area\noutflows_df &lt;- df20 %&gt;% \n  filter(start_polygon_name != end_polygon_name) %&gt;% \n  group_by(start_polygon_name) %&gt;% \n  dplyr::summarise(\n    mean_outflow = mean(n_crisis, na.rm = T)\n    ) %&gt;% \n  ungroup()\n\n# mean inflow by area\ninflows_df &lt;- df20 %&gt;% \n  filter(start_polygon_name != end_polygon_name) %&gt;% \n  group_by(end_polygon_name) %&gt;% \n  dplyr::summarise(\n    mean_inflow = mean(n_crisis, na.rm = T)\n    ) %&gt;% \n  ungroup()\n\n# combine data frames\nnetflow_df &lt;- cbind(inflows_df, outflows_df)\n\n# mean netflow by area\nnetflow_df &lt;- netflow_df %&gt;% \n  mutate(\n    mean_netflow = mean_inflow - mean_outflow\n  ) %&gt;% \n  select(start_polygon_name, end_polygon_name, mean_inflow, mean_outflow, mean_netflow)\n\nhead(netflow_df)\n\n  start_polygon_name end_polygon_name mean_inflow mean_outflow mean_netflow\n1        Antofagasta      Antofagasta    63.27513     66.00571    -2.730582\n2             Arauco           Arauco    91.14286     93.48276    -2.339901\n3              Arica            Arica    17.44444     20.16379    -2.719349\n4              Aysen            Aysen    28.47297     27.15385     1.319127\n5            Bío-Bío          Bío-Bío    77.86880     79.00000    -1.131195\n6          Cachapoal        Cachapoal   110.54008    112.71988    -2.179800\n\n\nWe now have a set of useful indicators. However, analysing and identifying patterns in large tables is challenging. Effective visualisations are needed to enable this.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data manipulation and visualisation</span>"
    ]
  },
  {
    "objectID": "02b_geomanipulation.html#geospatial-visualisation",
    "href": "02b_geomanipulation.html#geospatial-visualisation",
    "title": "2  Spatial data manipulation and visualisation",
    "section": "2.4 Geospatial visualisation",
    "text": "2.4 Geospatial visualisation\nThis section focuses on illustrating how to produce geospatial visualisations in R. They allow us to create more meaninful representations of the data and add context to our analysis. A carefully crafted map can be an effective way of communicating complex information. Design issues include poor placement, size and readability of text and careless selection of colors. Have a look the style guide of the Journal of Maps for details, and also at the use of colours for effective communication. We recommend this piece by Crameri, Shephard, and Heron (2020) and Maceachren and Kraak (1997).\n\n\n\n\n\n\nNote\n\n\n\nFor colour palettes, we recommend the following resources:\n\nthe R packages viridis and RColorBrewer\nthe website color brewer 2.0\na publication by Crameri, Shephard, and Heron (2020)\n\n\n\nA generally useful starting point for any analysis involving geospatial or nongeospatial data is to create graphical data representations using histograms or kernel density histograms. They are helpful to easily have an understanding of the distribution of mobility indicators. The histogram below reveals that while most areas report an average net mobility balance of zero, some areas display losses or gains of over 10 or 20 movements, respectively.\n\n\n\n\n\n\nNote\n\n\n\nggplot is a primary tool for data visualisation in R. ggplot has a basic structure of three components:\n\nThe data: ggplot( data = data frame)\nGeometries: geom_xxx( . )\nAesthetic mapping: aes(x=variable, y=variable)\n\nThen you can work on the cosmetics of the figure such as changing the theme, axis, labels, etc.\n\n\n\nggplot(data = netflow_df) + # input the data\n  geom_density(aes(x = mean_netflow), # specify type of geom and aesthetics\n               alpha=0.5, \n               colour=\"darkblue\", \n               linewidth = 2\n               ) +\n  theme_tufte() + # choose the theme\n  theme(axis.text.y = element_text(size = 12), # edit labels format\n        axis.text.x = element_text(size = 12),\n        axis.title=element_text(size=14)\n                    ) + \n  labs(y = \"Density\", # label axis\n       x = \"Average net movement count\")\n\n\n\n\n\n\n\n\nBefore we get to map our data, there are some key elements we need to cover first. A first key thing you always need to know is the CRS of our data and ensuring there is a commonly shared CRS across our data if we are using multiple layers of geospatial data (i.e. flows and polygons). For mapping, we will be using a planar system, as opposed to a spherical system. We set our default CRS.\n\n# set crs\ncrs_default = \"EPSG:4326\"\n\nTo ensure you get a full understanding of the process of mapping, we will roll back and start from the beginning. We will re-read the data and re-compute the indicators, but this time we will convert our movement dataset into a spatial data frame by using the latitude and longitude of the origins as coordinates. This information will be used to convert our movement data frame into a spatial data frame using the latitude and longitude of the origins as geometry.\n\ndf20 &lt;- readRDS(\"./data/fb/movement_adm/2020_04.rds\") %&gt;% \n  mutate(GEOMETRY = NULL) %&gt;% \n  dplyr::filter(country == \"CL\") %&gt;% \n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), \n                                      crs = crs_default)\n\nglimpse(df20)\n\nRows: 29,491\nColumns: 18\n$ date_time                    &lt;chr&gt; \"2020-04-01 00:00\", \"2020-04-01 00:00\", \"…\n$ start_polygon_id             &lt;chr&gt; \"60845\", \"60862\", \"60845\", \"60862\", \"6086…\n$ start_polygon_name           &lt;chr&gt; \"Antofagasta\", \"Cardenal Caro\", \"Antofaga…\n$ end_polygon_id               &lt;chr&gt; \"60847\", \"60890\", \"60846\", \"60863\", \"6089…\n$ end_polygon_name             &lt;chr&gt; \"Tocopilla\", \"Melipilla\", \"El Loa\", \"Colc…\n$ length_km                    &lt;dbl&gt; 139.7543134, 24.0764953, 100.3339392, 24.…\n$ tile_size                    &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1…\n$ country                      &lt;chr&gt; \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\", \"CL\",…\n$ level                        &lt;chr&gt; \"LEVEL3\", \"LEVEL3\", \"LEVEL3\", \"LEVEL3\", \"…\n$ n_crisis                     &lt;dbl&gt; 79, 18, 320, 71, NA, 369, NA, 20, 11, NA,…\n$ n_baseline                   &lt;dbl&gt; 59.50, 25.50, 671.00, 132.75, NA, 559.25,…\n$ n_difference                 &lt;dbl&gt; 19.50, -7.50, -351.00, -61.75, NA, -190.2…\n$ percent_change               &lt;dbl&gt; 32.231405, -28.301887, -52.232143, -46.16…\n$ is_statistically_significant &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ z_score                      &lt;dbl&gt; 2.0148470, -0.6837043, -4.0000000, -1.261…\n$ end_lat                      &lt;dbl&gt; -22.06894, -33.75413, -22.81611, -34.7036…\n$ end_lon                      &lt;dbl&gt; -69.60081, -71.19829, -68.20015, -71.0631…\n$ geometry                     &lt;POINT [°]&gt; POINT (-69.56718 -24.32798), POINT …\n\n\nWe will also re-read our province boundary data and ensure it is on the same CRS.\n\nshp_pro &lt;- read_sf(\"./data/shp/adm/province/PROVINCIAS_2020.shp\") %&gt;% \n  st_simplify(preserveTopology =T,\n              dTolerance = 1000) %&gt;%  # 1km\n  sf::st_make_valid() %&gt;% \n  st_transform(crs_default)\n\nOnce we have read our input data, we will re-compute a few of mobility indicators.\n\n# mean outflow by area\noutflows_df &lt;- df20 %&gt;% \n  filter(start_polygon_name != end_polygon_name) %&gt;% \n  group_by(start_polygon_name) %&gt;% \n  dplyr::summarise(\n    mean_outflow = mean(n_crisis, na.rm = T)\n    ) %&gt;% \n  ungroup()\n\n# mean inflow by area\ninflows_df &lt;- df20 %&gt;% \n  filter(start_polygon_name != end_polygon_name) %&gt;% \n  group_by(end_polygon_name) %&gt;% \n  dplyr::summarise(\n    mean_inflow = mean(n_crisis, na.rm = T)\n    ) %&gt;% \n  ungroup() %&gt;% \n  st_drop_geometry()\n\n# combine data frames\nnetflow_df &lt;- cbind(outflows_df, inflows_df)\n\n# mean netflow by area\nnetflow_df &lt;- netflow_df %&gt;% \n  mutate(\n    mean_netflow = mean_inflow - mean_outflow\n  ) %&gt;% \n  dplyr::select(start_polygon_name, mean_inflow, mean_outflow, mean_netflow, geometry) %&gt;% \n  rename(\n    polygon_name = start_polygon_name\n  ) \n\nhead(netflow_df)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -73.58022 ymin: -45.45868 xmax: -69.56718 ymax: -18.71308\nGeodetic CRS:  WGS 84\n  polygon_name mean_inflow mean_outflow mean_netflow\n1  Antofagasta    63.27513     66.00571    -2.730582\n2       Arauco    91.14286     93.48276    -2.339901\n3        Arica    17.44444     20.16379    -2.719349\n4        Aysen    28.47297     27.15385     1.319127\n5      Bío-Bío    77.86880     79.00000    -1.131195\n6    Cachapoal   110.54008    112.71988    -2.179800\n                     geometry\n1 POINT (-69.56718 -24.32798)\n2  POINT (-73.3454 -37.72772)\n3 POINT (-69.85541 -18.71308)\n4 POINT (-73.58022 -45.45868)\n5 POINT (-71.92786 -37.53828)\n6  POINT (-70.7088 -34.28421)\n\n\nNow let’s start mapping our data. Let’s do this by stage so you understand the key building blocks of this process. We first drawn the polygons representing the provinces of Chile.\n\np &lt;- ggplot() + \n  geom_sf(data = shp_pro,\n          color = \"gray60\", \n          size = 0.1)\nlast_plot()\n\n\n\n\n\n\n\n\nLet’s add the centroids of the origins based on our movement data.\n\np &lt;- p +\n  geom_point(data = netflow_df,\n    aes(geometry = geometry),\n    stat = \"sf_coordinates\"\n  ) \nlast_plot()\n\n\n\n\n\n\n\n\nThen we can remove the axes or background here:\n\np + theme_void()\n\n\n\n\n\n\n\n\nThe map is placed towards the right. We can centre the map by adjusting the bounding box of the shapefile. We can do this by obtaining the current bounding box and adjusting the x left limit.\n\nbbox_new &lt;- st_bbox(shp_pro) # current bounding box\n\nxrange &lt;- bbox_new$xmax - bbox_new$xmin # range of x values\nyrange &lt;- bbox_new$ymax - bbox_new$ymin # range of y values\n\nbbox_new[1] &lt;- bbox_new[1] + (0.6 * xrange) # xmin - left\n#bbox_new[3] &lt;- bbox_new[3] + (0.5 * xrange) # xmax - right\n#bbox_new[2] &lt;- bbox_new[2] - (0.5 * yrange) # ymin - bottom\n#bbox_new[4] &lt;- bbox_new[4] + (0.5 * yrange) # ymax - top\n\nbbox_new &lt;- bbox_new %&gt;%  # take the bounding box ...\n  st_as_sfc() # ... and make it a sf polygon\n\nggplot() + \n  geom_sf(data = shp_pro,\n          color = \"gray60\", \n          size = 0.1) +\n  geom_point(data = netflow_df,\n    aes(geometry = geometry),\n    stat = \"sf_coordinates\",\n    size = .1\n  ) +\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], # min & max of x values\n           ylim = st_coordinates(bbox_new)[c(2,3),2]) + # min & max of y values\n  theme_void()\n\n\n\n\n\n\n\n\nSomething to note at this point is that two of the centroids lie outside the provincial polygons. This is difficult to visualise from the current map. We will find out this in a different way. We run a spatial join of the provincial polygons and our movement data frame, and check the provinces which were not matched. Below we can observe the result of this process. The provinces of Magallanes and Valparaíso were unmatched. That is because their centroids lie outside the polygon area. In the case of Valparaiso, islands associated with this province force the centroid to be position on the Pacific. In the case of Magallanes, the unusual shape of the province surrounded by fjords forces the centroid to be on the Pacific as well.\n\n# spatial join\nmob_indicators &lt;- st_join(shp_pro, netflow_df)\n# check if all polygons are matched\nnetflow_df$check &lt;- netflow_df$polygon_name %in% mob_indicators$polygon_name\n# identify unmatched polygons\nnetflow_df %&gt;% select(polygon_name, check) %&gt;% \n  filter(check == \"FALSE\")\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -71.75919 ymin: -53.0857 xmax: -71.70694 ymax: -33.14564\nGeodetic CRS:  WGS 84\n  polygon_name check                    geometry\n1   Magallanes FALSE  POINT (-71.70694 -53.0857)\n2   Valparaíso FALSE POINT (-71.75919 -33.14564)\n\n\nWe therefore adjust these centriods and force them to be within the polygons of provinces.\n\n# get coordinates\ncoordinates &lt;- st_coordinates(netflow_df) \n# combine data frames\nnetflow_df &lt;- cbind(netflow_df, coordinates) %&gt;% \n  rename(\n    long = X, \n    lat = Y\n  )\n# list province to be adjusted\nprovince_name &lt;- c(\"Valparaíso\", \"Magallanes\")\n# adjust the centroid\nfor (area in 1:2) {\n  long &lt;- netflow_df %&gt;% \n    st_drop_geometry() %&gt;% \n    dplyr::filter(polygon_name == province_name[area]) %&gt;% \n    select(long) %&gt;% \n    as.numeric()\n  \n  lat &lt;- netflow_df %&gt;% \n    st_drop_geometry() %&gt;%\n    dplyr::filter(polygon_name == province_name[area]) %&gt;% \n    select(lat) %&gt;% \n    as.numeric()\n\n  st_geometry(netflow_df[netflow_df$polygon_name == province_name[area], ]) &lt;-  st_sfc(st_point(c( long * 0.98, lat * 1 )))\n}\n\nWe re-join the polygon boundary data and our movement data frame and check the results. We can now see that all spatial units were matched.\n\nmob_indicators &lt;- st_join(shp_pro, netflow_df, \n                          st_intersects)\n\nnetflow_df$check &lt;- netflow_df$polygon_name %in% mob_indicators$polygon_name\n\nnetflow_df %&gt;% select(polygon_name, check) %&gt;% \n  filter(check == \"FALSE\")\n\nSimple feature collection with 0 features and 2 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] polygon_name check        geometry    \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nSo far, we have only wrangled the data so we can start out mapping and spatial analysis.\n\n2.4.1 Choropleths\nWe first focus on choropleths. Choropleths are thematic maps. They are easy to create but also to get wrong. We will look at a set of the principles you can follow to create effective choropleth maps. Here three more questions to consider:\n\nWhat is being plotted?\nWhat is the target audience?\nWhat degree of interactivity we want to offer?\n\nWe will create maps of netflows, inflows and outflows. We will divide the data into categories. Categorising the data often facilites the identification of spatial patterns. There are different forms of organising these categories using quantiles, natural break points or more sophisticated clustering approaches. We will keep this simple and use seven categories. Feel free to explore the impacts on the map as these categories change.\n\n# set categories\nmob_indicators &lt;- mob_indicators %&gt;% \n  mutate(\n    netflow_class = mean_netflow %&gt;% cut(7, dig.lab = 3),\n    inflow_class = mean_inflow %&gt;% cut(7, dig.lab = 3),\n    outflow_class = mean_outflow %&gt;% cut(7, dig.lab = 3)\n  ) \n\n# adjust labels for netflows\nnetflow_labels &lt;- levels(mob_indicators$netflow_class)\nnetflow_labels &lt;- gsub(\"\\\\(|\\\\]\", \"\", netflow_labels)\nlevels(mob_indicators$netflow_class) &lt;- netflow_labels\n\n# adjust labels for inflows\ninflow_labels &lt;- levels(mob_indicators$inflow_class)\ninflow_labels &lt;- gsub(\"\\\\(|\\\\]\", \"\", inflow_labels)\nlevels(mob_indicators$inflow_class) &lt;- inflow_labels\n\n# adjust labels for netflows\noutflow_labels &lt;- levels(mob_indicators$outflow_class)\noutflow_labels &lt;- gsub(\"\\\\(|\\\\]\", \"\", outflow_labels)\nlevels(mob_indicators$outflow_class) &lt;- outflow_labels\n\n# change geometry\nshp_reg &lt;- shp_reg %&gt;% st_transform(crs_default)\n\nBelow we use ggplot to create our maps.\n\n# map netflows\nnetflow_plot &lt;- ggplot(data = mob_indicators, aes(fill = netflow_class)) +\n  geom_sf(col = \"white\", size = .1) +\n  coord_sf() +\n  scale_fill_brewer(palette = \"RdBu\", direction = -1) +\n  scale_color_manual(labels = netflow_labels) +\n  theme_map() +\n  theme(plot.title = element_text(size = 22, face = \"bold\"),\n        legend.position = \"none\") +\n  labs(title = \"(a) Netflow\",\n       fill = \"Net count of moves\") +\n  theme_void() + \n  geom_sf(data = shp_reg,\n          col = \"grey70\", \n          size = .5,\n          fill = \"transparent\") +\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], \n           ylim = st_coordinates(bbox_new)[c(2,3),2]) \n\n# map inflows\ninflow_plot &lt;- ggplot(data = mob_indicators, aes(fill = inflow_class)) +\n  geom_sf(col = \"white\", size = .1) +\n  coord_sf() +\n  scale_fill_brewer(palette = \"PuRd\", direction = 1) +\n  theme_map() +\n  theme(plot.title = element_text(size = 22, face = \"bold\"),\n        legend.position = \"none\") +\n  labs(title = \"(b) Inflow\",\n       fill = \"Count of moves\") +\n  theme_void() + \n  geom_sf(data = shp_reg,\n          col = \"grey70\", \n          size = .5,\n          fill = \"transparent\") +\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], \n           ylim = st_coordinates(bbox_new)[c(2,3),2]) \n\n# map outflows\noutflow_plot &lt;- ggplot(data = mob_indicators, aes(fill = outflow_class)) +\n  geom_sf(col = \"white\", size = .1) +\n  coord_sf() +\n  scale_fill_brewer(palette = \"PuBu\", direction = 1) +\n  theme_map() +\n  theme(plot.title = element_text(size = 22, face = \"bold\"),\n        legend.position = \"none\") +\n  labs(title = \"(c) Outflow\",\n       fill = \"Count of moves\") +\n  theme_void() +\n    geom_sf(data = shp_reg,\n          col = \"grey70\", \n          size = .5,\n          fill = \"transparent\") +\n  coord_sf(xlim = st_coordinates(bbox_new)[c(1,2),1], \n           ylim = st_coordinates(bbox_new)[c(2,3),2]) \n\n# combine plots\nnetflow_plot + inflow_plot + outflow_plot\n\n\n\n\n\n\n\n\nThe maps display the average count of movement across provinces in Chile. In relative terms, they indicate that some of central provinces experienced larger losses compared to northern and southern provinces. Central provinces also display the largest average counts of movement. This is expected as these areas concentrate most of the Chilean population, and larger population centres are expected to generate more movement.\n\n\n2.4.2 Interactive mapping\nAn alternative way to analyse the data is using interactive maps. Interactive maps provide greater flexibility to explore the data by adding a base map for greater context. We use tmap for this task. tmap is a great package for mapping and you should probably add it to your toolbox if you are working on geographic information systems using R.\nWe map the average net count of movement for March 2020. We first enable interactivity by running tmap_mode(\"view\") and then create the map. Using the menu on the left top of the map below, we can zoom in and out to explore the map and also change the base layer. We can also display or hide the colouring layers of our indicator. This allows to see the names of the areas experiencing net gains and losses of moves.\n\n# create a map\ntmap_mode(\"view\") # enable interactivity\ntm_shape(mob_indicators) + # input data\n    tm_fill(\"mean_netflow\", # draw and fill polygons\n          palette = \"RdBu\",\n          title = \"Netflows\")\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe use ttm() to switch off the interactive functionality of tmap.\n\n\n\n# switch to other mode: \"view\"\nttm()\n\nℹ tmap mode set to \"plot\".\nℹ switch back to \"view\" mode with `tmap::ttm()`\n\n\n\n\n2.4.3 Flow mapping\nAn additional key dimension of the analysis of origin-destination flow data is visually understanding the relationship between origins and destinations. This helps understanding the extent of interaction of individual areas with the rest of the country, and also informs how changes in a specific area may impact others - see for example Rowe and Patias (2020) and Rowe (2022). Flow maps are often used to visualise the complex network of mobility flows between areas. A recently developed tool for this end is mapbox. If you want to use mapbox you need to sign up. You can create an account for free to use some basic products. We can use mapbox through the mapdeck package in R.\nSo you have a start-to-end understanding of the process, we will begin from loading the input movement data. We compute the average count by origin-destination pair for April 2020, and prepare a data frame containing origin and destination coordinates.\n\n# read data\ndf20 &lt;- readRDS(\"./data/fb/movement_adm/2020_04.rds\") %&gt;% \n  mutate(GEOMETRY = NULL) %&gt;% \n  dplyr::filter(country == \"CL\") \n\n# compute mean move by origin-destination pair\nflow_df20 &lt;- df20 %&gt;% \n  filter(start_polygon_name != end_polygon_name) %&gt;% \n  group_by(start_polygon_name, end_polygon_name) %&gt;% \n  dplyr::summarise(\n    mean_flow = mean(n_crisis, na.rm = T)\n    ) %&gt;% \n  ungroup()\n\n# create a coordinate data frame for origins\norigin_coordinate_df &lt;- df20 %&gt;% \n  dplyr::select( c(start_polygon_name, start_lat, start_lon)) %&gt;% \n  distinct()\n\n# create a coordinate data frame for destinations\ndestination_coordinate_df &lt;- df20 %&gt;% \n  dplyr::select( c(end_polygon_name, end_lat, end_lon)) %&gt;% \n  distinct()\n\n# join coordinates for origins and destinations\nflow_df20 &lt;- left_join(flow_df20, origin_coordinate_df, by = c(\"start_polygon_name\" = \"start_polygon_name\"))\nflow_df20 &lt;- left_join(flow_df20, destination_coordinate_df, by = c(\"end_polygon_name\" = \"end_polygon_name\"))\n\nFor our illustration, we will focus on the province of Santiago.\n\ndf_santiago &lt;- flow_df20 %&gt;% dplyr::filter(start_polygon_name == \"Santiago\")\n\nhead(df_santiago)\n\n# A tibble: 6 × 7\n  start_polygon_name end_polygon_name mean_flow start_lat start_lon end_lat\n  &lt;chr&gt;              &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Santiago           Antofagasta           33.3     -33.4     -70.5   -24.3\n2 Santiago           Arauco               NaN       -33.4     -70.5   -37.7\n3 Santiago           Arica                 17.8     -33.4     -70.5   -18.7\n4 Santiago           Bío-Bío               18.5     -33.4     -70.5   -37.5\n5 Santiago           Cachapoal            292.      -33.4     -70.5   -34.3\n6 Santiago           Cardenal Caro         13.3     -33.4     -70.5   -34.3\n# ℹ 1 more variable: end_lon &lt;dbl&gt;\n\n\nTo use mapbox, you will need to sign up for an account on the mapbox website. For this workshop, we have generated a key to share but we strongly suggest to create an account as the key will last for a certain period of time. The map generated is interactive. If you can explore it by clicking on it and moving your mouse. If you can also zoom in and out by double-clicking.\nA striking pattern of the map below is the high degree of connectivity of Santiago with the rest of the country. Flows from Santiago not only extend to proximate and adjacent provinces but extend to far provinces in the north and south of the country.\n\nkey &lt;- my_key ## put your own token here\n\nflowmap &lt;- mapdeck( token = key, style = mapdeck_style(\"dark\"),\n                location = c(-3.7, 40.4), zoom = 6, pitch = 45) %&gt;%\n  add_arc(\n    data = df_santiago,\n    layer_id = \"arc_layer\",\n    origin = c(\"start_lon\", \"start_lat\"),\n    destination = c(\"end_lon\", \"end_lat\"),\n    # stroke_from = \"start_polygon_name\",\n    # stroke_to = \"end_polygon_name\",\n    # stroke_width = \"stroke\",\n    palette = \"reds\",\n    legend = list( stroke_from = F, stroke_to = F ),\n  )\n\n# plot the interactive map\nflowmap\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate a flow map for your province of interest.\n\n\n\n\n\n\nAppelhans, Tim, Florian Detsch, Christoph Reudenbach, and Stefan Woellauer. 2022. Mapview: Interactive Viewing of Spatial Data in r. https://github.com/r-spatial/mapview.\n\n\nBaddeley, Adrian, Ege Rubak, and Rolf Turner. 2015. Spatial Point Patterns: Methodology and Applications with r. CRC press.\n\n\nBaddeley, Adrian, Rolf Turner, and Ege Rubak. 2022. Spatstat: Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests. http://spatstat.org/.\n\n\nBivand, Roger. 2022. Spdep: Spatial Dependence: Weighting Schemes, Statistics.\n\n\nBivand, Roger, and Gianfranco Piras. 2022. Spatialreg: Spatial Regression Analysis. https://CRAN.R-project.org/package=spatialreg.\n\n\nCrameri, Fabio, Grace E. Shephard, and Philip J. Heron. 2020. “The Misuse of Colour in Science Communication.” Nature Communications 11 (1). https://doi.org/10.1038/s41467-020-19160-7.\n\n\nDunnington, Dewey, Edzer Pebesma, and Ege Rubak. 2023. S2: Spherical Geometry Operators Using the S2 Geometry Library. https://CRAN.R-project.org/package=s2.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. CRC Press.\n\n\nMaceachren, Alan M., and Menno-Jan Kraak. 1997. “Exploratory Cartographic Visualization: Advancing the Agenda.” Computers & Geosciences 23 (4): 335–43. https://doi.org/10.1016/s0098-3004(97)00018-6.\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\n———. 2022a. Sf: Simple Features for r. https://CRAN.R-project.org/package=sf.\n\n\n———. 2022b. Stars: Spatiotemporal Arrays, Raster and Vector Data Cubes. https://CRAN.R-project.org/package=stars.\n\n\n———. 2023. Lwgeom: Bindings to Selected Liblwgeom Functions for Simple Features. https://github.com/r-spatial/lwgeom/.\n\n\nPebesma, Edzer J. 2004. “Multivariable Geostatistics in S: The Gstat Package.” Computers & Geosciences 30 (7): 683–91. https://doi.org/10.1016/j.cageo.2004.03.012.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in r. CRC Press.\n\n\nPebesma, Edzer, and Benedikt Graeler. 2022. Gstat: Spatial and Spatio-Temporal Geostatistical Modelling, Prediction and Simulation. https://github.com/r-spatial/gstat/.\n\n\nRowe, Francisco. 2022. “Using Digital Footprint Data to Monitor Human Mobility and Support Rapid Humanitarian Responses.” Regional Studies, Regional Science 9 (1): 665–68. https://doi.org/10.1080/21681376.2022.2135458.\n\n\nRowe, Francisco, Alessia Calafiore, Daniel Arribas-Bel, Krasen Samardzhiev, and Martin Fleischmann. 2022. “Urban Exodus? Understanding Human Mobility in Britain During the COVID-19 Pandemic Using Meta-Facebook Data.” Population, Space and Place 29 (1). https://doi.org/10.1002/psp.2637.\n\n\nRowe, Francisco, Robin Lovelace, and Adam Dennett. 2022. “Spatial Interaction Modelling: A Manifesto.” http://dx.doi.org/10.31219/osf.io/xcdms.\n\n\nRowe, Francisco, and Nikos Patias. 2020. “Mapping the Spatial Patterns of Internal Migration in Europe.” Regional Studies, Regional Science 7 (1): 390–93. https://doi.org/10.1080/21681376.2020.1811139.\n\n\nTennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06.\n\n\n———. 2022. Tmap: Thematic Maps. https://github.com/r-tmap/tmap.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spatial data manipulation and visualisation</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html",
    "href": "02c_point-pattern.html",
    "title": "3  Point data analysis",
    "section": "",
    "text": "3.1 Dependencies\nThis Chapter aims to provide an introduction on how to manipulate and analyse geospatial point data.\nWe will use the following libraries.\n# data manipulation, transformation and visualisation\nlibrary(tidyverse)\n\n# spatial data manipulation\nlibrary(sf)\nlibrary(sp)\n\n# data visualisation\nlibrary(gridExtra)\n\n# basemap\nlibrary(basemapR)\n\n# interpolation\nlibrary(gstat)\nlibrary(hexbin)",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#data",
    "href": "02c_point-pattern.html#data",
    "title": "3  Point data analysis",
    "section": "3.2 Data",
    "text": "3.2 Data\nFor this session, we will use the set of Airbnb properties for San Diego (US), borrowed from the “Geographic Data Science with Python” book (see here for more info on the dataset source). This covers the point location of properties advertised on the Airbnb website in the San Diego region.\nLet us start by reading the data, which comes in a GeoJSON:\n\ndb &lt;- st_read(\"data/abb_sd/regression_db.geojson\")\n\nReading layer `regression_db' from data source \n  `/Users/franciscorowe/Library/CloudStorage/Dropbox/Francisco/Research/presentations/2025/montevideo-2/github/smds-pop/data/abb_sd/regression_db.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 6110 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2812 ymin: 32.57349 xmax: -116.9553 ymax: 33.08311\nGeodetic CRS:  WGS 84\n\n\nWe can then examine the columns of the table with the colnames method:\n\ncolnames(db)\n\n [1] \"accommodates\"       \"bathrooms\"          \"bedrooms\"          \n [4] \"beds\"               \"neighborhood\"       \"pool\"              \n [7] \"d2balboa\"           \"coastal\"            \"price\"             \n[10] \"log_price\"          \"id\"                 \"pg_Apartment\"      \n[13] \"pg_Condominium\"     \"pg_House\"           \"pg_Other\"          \n[16] \"pg_Townhouse\"       \"rt_Entire_home.apt\" \"rt_Private_room\"   \n[19] \"rt_Shared_room\"     \"geometry\"          \n\n\nWe will focus on two main elements of the table: (1) the spatial dimension (as stored in the point coordinates), and (2) the nightly price values, expressed in USD. To get a sense of what they look like first, let us plot both. We can get a quick look at the non-spatial distribution of house values with the following commands:\n\n# Create the histogram\nggplot(data = db, aes(x = price)) +\n  geom_histogram(binwidth = 50, # adjust the bin width\n                 colour = \"transparent\",\n                 fill = \"#00008B\") +\n  labs(title = \"Price per night\",\n       x = \"Price (in USD)\",\n       y = \"Frequency\") +\n  theme_plot_tufte() # use a clean, minimal theme\n\n\n\n\n\n\n\n\nThis basically shows there is a lot of values concentrated around the lower end of the distribution but a few very large ones. A usual transformation to shrink these differences is to take logarithms. The original table already contains an additional column with the logarithm of each price.\n\n# Create the histogram\nggplot(data = db, aes(x = log_price)) +\n  # adjust the bin width\n  geom_histogram(binwidth = .05, \n                 colour = \"transparent\",\n                 fill = \"#00008B\") +\n  labs(title = \"Price per night\",\n       x = \"Price (in USD)\",\n       y = \"Frequency\") +\n  theme_plot_tufte() # use a clean, minimal theme\n\n\n\n\n\n\n\n\nTo obtain the spatial distribution of these houses, we need to focus on the geometry. The easiest, quickest (and also “dirtiest”) way to get a sense of what the data look like over space is using the plot base function:\n\nplot(st_geometry(db))\n\n\n\n\n\n\n\n\nNow this has the classic problem of cluttering: some portions of the map have so many points that we can’t tell what the distribution is like. To get around this issue, there are two solutions: binning and smoothing.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#binning",
    "href": "02c_point-pattern.html#binning",
    "title": "3  Point data analysis",
    "section": "3.3 Binning",
    "text": "3.3 Binning\nThe two-dimensional sister of histograms are binning maps: we divide each of the two dimensions into “buckets”, and count how many points fall within each bucket. Unlike histograms, we encode that count with a color gradient rather than a bar chart over an additional dimension (for that, we would need a 3D plot). These “buckets” can be squares (left) or hexagons (right):\n\n      # Squared binning\n# Set up plot\nsqbin &lt;- ggplot( ) + \n# Add 2D binning with the XY coordinates as\n# a dataframe\n  geom_bin2d(\n    data = as.data.frame( st_coordinates( db ) ), \n    aes( x = X, y = Y)\n  ) + \n  # set theme \n  theme_plot_tufte()\n      # Hex binning\n# Set up plot\nhexbin &lt;- ggplot() +\n# Add hex binning with the XY coordinates as\n# a dataframe \n  geom_hex(\n    data = as.data.frame( st_coordinates( db ) ),\n    aes( x = X, y = Y)\n  ) +\n# Use viridis for color encoding (recommended)\n  scale_fill_continuous( type = \"viridis\" ) +\n  theme_plot_tufte()\n      # Bind in subplots\ngrid.arrange( sqbin, hexbin, ncol = 2 )",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#kde",
    "href": "02c_point-pattern.html#kde",
    "title": "3  Point data analysis",
    "section": "3.4 KDE",
    "text": "3.4 KDE\nKernel Density Estimation (KDE) is a technique that creates a continuous representation of the distribution of a given variable, such as house prices. Although theoretically it can be applied to any dimension, usually, KDE is applied to either one or two dimensions.\n\n3.4.1 One-dimensional KDE\nKDE over a single dimension is essentially a contiguous version of a histogram. We can see that by overlaying a KDE on top of the histogram of logs that we have created before:\n\n# Create the base\nbase &lt;- ggplot(db, aes(x=log_price))\n# Histogram\nhist &lt;- base + \n  geom_histogram(bins=50, \n                 aes(y=..density..),\n                 colour = \"transparent\",\n                 fill = \"#00008B\")\n# Overlay density plot\nkde &lt;- hist + \n  geom_density(fill= \"#FFD700\", \n               alpha=0.3, \n               colour=\"#FFD700\") +\n  theme_plot_tufte()\nkde\n\n\n\n\n\n\n\n\nThe key idea is that we are smoothing out the discrete binning that the histogram involves. Note how the histogram is exactly the same as above shape-wise, but it has been rescalend on the Y axis to reflect probabilities rather than counts.\n\n\n3.4.2 Two-dimensional (spatial) KDE\nGeography, at the end of the day, is usually represented as a two-dimensional space where we locate objects using a system of dual coordinates: latitude and longitude in our case. Hence, here, we can use the same technique as above to obtain a smooth representation of the distribution of a two-dimensional variable. The crucial difference is that, instead of obtaining a curve as the output, we will create a surface, where intensity will be represented with a color gradient, rather than with the second dimension, as it is the case in the figure above.\nTo create a spatial KDE, we can use general tooling for non-spatial points.\n\n# Create the KDE surface\nkde &lt;- ggplot(data = db) +\n  stat_density2d_filled(alpha = 1,\n    data = as.data.frame(st_coordinates(db)), \n    aes(x = X, y = Y),\n    n = 100\n  ) +\n  # Tweak the color gradient\n  scale_color_viridis_c() +\n  # White theme\n  theme_plot_tufte() \nkde\n\n\n\n\n\n\n\n\nThis approach generates a surface that represents the density of dots, that is an estimation of the probability of finding a house transaction at a given coordinate. However, without any further information, they are hard to interpret and link with previous knowledge of the area. To bring such context to the figure, we can plot an underlying basemap, using a cloud provider such as Google Maps or, as in this case, OpenStreetMap. Before we can plot them with the online map, we need should ensure that our spatial data are on the same projection system, and reproject them if needed.\n\nbbox_db &lt;- st_bbox(db)\nggplot() +\n  base_map(bbox_db, increase_zoom = 2, basemap = \"positron\") +\n  #geom_sf(data = db, fill = NA) +\n  stat_density2d_filled(alpha = 0.7,\n    data = as.data.frame(st_coordinates(db)), \n    aes(x = X, y = Y),\n    n = 100\n  )",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#spatial-interpolation",
    "href": "02c_point-pattern.html#spatial-interpolation",
    "title": "3  Point data analysis",
    "section": "3.5 Spatial Interpolation",
    "text": "3.5 Spatial Interpolation\nThe previous section demonstrates how to visualize the distribution of a set of spatial objects represented as points. In particular, given a bunch of house locations, it shows how one can effectively visualize their distribution over space and get a sense of the density of occurrences. Such visualization, because it is based on KDE, is based on a smooth continuum, rather than on a discrete approach (as a choropleth would do, for example).\nMany times however, we are not particularly interested in learning about the density of occurrences, but about the distribution of a given value attached to each location. Think for example of weather stations and temperature: the location of the stations is no secret and rarely changes, so it is not of particular interest to visualize the density of stations; what we are usually interested instead is to know how temperature is distributed over space, given we only measure it in a few places. One could argue the example we have been working with so far, house prices in AirBnb, fits into this category as well: although where a house is advertised may be of relevance, more often we are interested in finding out what the “surface of price” looks like. Rather than where are most houses being advertised? we usually want to know where the most expensive or most affordable houses are located.\nIn cases where we are interested in creating a surface of a given value, rather than a simple density surface of occurrences, KDE cannot help us. In these cases, what we are interested in is spatial interpolation, a family of techniques that aim at exactly that: creating continuous surfaces for a particular phenomenon (e.g. temperature, house prices) given only a finite sample of observations. Spatial interpolation is a large field of research that is still being actively developed and that can involve a substantial amount of mathematical complexity in order to obtain the most accurate estimates possible1. In this chapter, we will introduce the simplest possible way of interpolating values, hoping this will give you a general understanding of the methodology and, if you are interested, you can check out further literature. For example, Banerjee, Carlin, and Gelfand (2014) or Cressie (2015) are hard but good overviews.\n\n3.5.1 Inverse Distance Weight (IDW) interpolation\nThe technique we will cover here is called Inverse Distance Weighting, or IDW for convenience. Brunsdon and Comber (2015) offer a good description:\n\nIn the inverse distance weighting (IDW) approach to interpolation, to estimate the value of \\(z\\) at location \\(x\\) a weighted mean of nearby observations is taken […]. To accommodate the idea that observations of \\(z\\) at points closer to \\(x\\) should be given more importance in the interpolation, greater weight is given to these points […]\n— Page 204\n\nThe math2 is not particularly complicated and may be found in detail elsewhere (the reference above is a good starting point), so we will not spend too much time on it. More relevant in this context is the intuition behind. The idea is that we will create a surface of house price by smoothing many values arranged along a regular grid and obtained by interpolating from the known locations to the regular grid locations. This will give us full and equal coverage to soundly perform the smoothing.\nEnough chat, let’s code3.\nFrom what we have just mentioned, there are a few steps to perform an IDW spatial interpolation:\n\nCreate a regular grid over the area where we have house transactions.\nObtain IDW estimates for each point in the grid, based on the values of \\(k\\) nearest neighbors.\nPlot a smoothed version of the grid, effectively representing the surface of house prices.\n\nLet us go in detail into each of them. First, let us set up a grid for the extent of the bounding box of our data:\n\nsd.grid &lt;- db %&gt;%\n  st_bbox() %&gt;%\n  st_as_sfc() %&gt;%\n  st_make_grid(\n    n = 100,\n    what = \"centers\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  cbind(., st_coordinates(.))\n\nThe object sd.grid is a regular grid with 10,000 (\\(100 \\times 100\\)) equally spaced cells:\n\nsd.grid\n\nSimple feature collection with 10000 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2795 ymin: 32.57604 xmax: -116.9569 ymax: 33.08056\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           X        Y                          x\n1  -117.2795 32.57604 POINT (-117.2795 32.57604)\n2  -117.2763 32.57604 POINT (-117.2763 32.57604)\n3  -117.2730 32.57604  POINT (-117.273 32.57604)\n4  -117.2698 32.57604 POINT (-117.2698 32.57604)\n5  -117.2665 32.57604 POINT (-117.2665 32.57604)\n6  -117.2632 32.57604 POINT (-117.2632 32.57604)\n7  -117.2600 32.57604   POINT (-117.26 32.57604)\n8  -117.2567 32.57604 POINT (-117.2567 32.57604)\n9  -117.2535 32.57604 POINT (-117.2535 32.57604)\n10 -117.2502 32.57604 POINT (-117.2502 32.57604)\n\n\nNow, sd.grid only contain the location of points to which we wish to interpolate. That is, we now have our “target” geography for which we’d like to have AirBnb prices, but we don’t have price estimates. For that, on to the IDW, which will generate estimates for locations in sd.grid based on the observed prices in our data frame. We can run:\n\nidw.hp &lt;- idw(\n  price ~ 1,         # Formula for IDW\n  locations = db,    # Initial locations with values\n  newdata=sd.grid,   # Locations we want predictions for\n  nmax = 150         # Limit the number of neighbours for IDW\n)\n\n[inverse distance weighted interpolation]\n\n\nBoom! We’ve got it. Let us pause for a second to see how we just did it. First, we pass the IDW formula specifying the functional representation we are using to model house prices. The left component represents the variable we want to explain or dependent variable, while everything to its right captures the explanatory variables. Since we are considering the simplest possible case, we do not have further variables to add, so we simply incorporate an intercept. Then we specify the original locations for which we do have house prices in our original data frame, and the points where we want to interpolate the house prices: the recently created grid object above. One more note: by default, IDW uses all the available observations, weighted by distance, to provide an estimate for a given point. Keep an eye as these default settings may vary by algorithm. You can modify that and restrict the maximum number of neighbors to consider. We consider the 150 nearest observations to each point4.\nThe resulting object is another spatial table, containing the interpolated values. As such, we can inspect it just as with any other of its kind. For example, to check out the top of the estimated table:\n\nhead(idw.hp)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2795 ymin: 32.57604 xmax: -117.2632 ymax: 32.57604\nGeodetic CRS:  WGS 84\n  var1.pred var1.var                   geometry\n1  295.6100       NA POINT (-117.2795 32.57604)\n2  295.1651       NA POINT (-117.2763 32.57604)\n3  296.5927       NA  POINT (-117.273 32.57604)\n4  288.2252       NA POINT (-117.2698 32.57604)\n5  281.5522       NA POINT (-117.2665 32.57604)\n6  268.3567       NA POINT (-117.2632 32.57604)\n\n\nThe column we will pay attention to is var1.pred. For a hypothetical house advertised at the location in the first row of point in sd.grid, the price IDW would guess it would cost, based on prices nearby, is the first element of column var1.pred in idw.hp.\n\n\n3.5.2 A surface of housing prices\nOnce we have the IDW object computed, we can plot it to explore the distribution, not of AirBnb locations in this case, but of house prices over the geography of San Diego. To do this using ggplot2, we first append the coordinates of each grid cell as columns of the table:\n\nidw.hp = idw.hp %&gt;%\n  cbind(st_coordinates(.))\n\nNow, we can visualise the surface using standard ggplot2 tools:\n\nggplot(idw.hp, aes(x = X, y = Y, fill = var1.pred)) +\n  geom_raster()\n\n\n\n\n\n\n\n\nAnd we can “dress it up” a bit further:\n\nggplot(idw.hp, aes(x = X, y = Y, fill = var1.pred)) +\n  geom_raster() +\n  scale_fill_viridis_b() +\n  theme_void() +\n  geom_sf(alpha=0)\n\n\n\n\n\n\n\n\nLooking at this, we can start to tell some patterns. To bring in context, it would be great to be able to add a basemap layer, as we did for the KDE. This is conceptually very similar to what we did above, starting by reprojecting the points and continuing by overlaying them on top of the basemap. However, technically speaking it is not possible because ggmap –the library we have been using to display tiles from cloud providers– does not play well with our own rasters (i.e. the price surface). At the moment, it is surprisingly tricky to get this to work, so we will park it for now5.\n\n\n3.5.3 “What should the next house’s price be?”\nThe last bit we will explore in this session relates to prediction for new values. Imagine you are a real state data scientist working for Airbnb and your boss asks you to give an estimate of how much a new house going into the market should cost. The only information you have to make such a guess is the location of the house. In this case, the IDW model we have just fitted can help you. The trick is realizing that, instead of creating an entire grid, all we need is to obtain an estimate of a single location.\nLet us say, a new house is going to be advertised on the coordinates X = -117.02259063720702, Y = 32.76511965117273 as expressed in longitude and latitude. In that case, we can do as follows:\n\npt &lt;- c(X = -117.02259063720702, Y = 32.76511965117273) %&gt;%\n  st_point() %&gt;%\n  st_sfc() %&gt;%\n  st_sf(crs = \"EPSG:4326\") %&gt;%\n  st_transform(st_crs(db))\nidw.one &lt;- idw(price ~ 1, locations=db, newdata=pt)\n\n[inverse distance weighted interpolation]\n\nidw.one\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.0226 ymin: 32.76512 xmax: -117.0226 ymax: 32.76512\nGeodetic CRS:  WGS 84\n  var1.pred var1.var                   geometry\n1  171.4141       NA POINT (-117.0226 32.76512)\n\n\nAnd, as show above, the estimated value is $171.41413346.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#questions",
    "href": "02c_point-pattern.html#questions",
    "title": "3  Point data analysis",
    "section": "3.6 Questions",
    "text": "3.6 Questions\nWe will be using the Madrid AirBnb dataset:\n\nmad_abb &lt;- st_read(\"data/assignment_1_madrid/madrid_abb.gpkg\")\n\nReading layer `madrid_abb' from data source \n  `/Users/franciscorowe/Library/CloudStorage/Dropbox/Francisco/Research/presentations/2025/montevideo-2/github/smds-pop/data/assignment_1_madrid/madrid_abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 18399 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -3.86391 ymin: 40.33243 xmax: -3.556 ymax: 40.56274\nGeodetic CRS:  WGS 84\n\n\nThis is fairly similar in spirit to the one from San Diego we have relied on for the chapter, although the column set is not exactly the same:\n\ncolnames(mad_abb)\n\n [1] \"price\"           \"price_usd\"       \"log1p_price_usd\" \"accommodates\"   \n [5] \"bathrooms_text\"  \"bathrooms\"       \"bedrooms\"        \"beds\"           \n [9] \"neighbourhood\"   \"room_type\"       \"property_type\"   \"WiFi\"           \n[13] \"Coffee\"          \"Gym\"             \"Parking\"         \"km_to_retiro\"   \n[17] \"geom\"           \n\n\nFor this set of questions, the only two columns we will need is geom, which contains the point geometries, and price_usd, which record the price of the AirBnb property in USD.\nWith this at hand, answer the following questions:\n\nCreate a KDE that represents the density of locations of AirBnb properties in Madrid\nUsing inverse distance weighting, create a surface of AirBnb prices\n\nThis chapter is based on the following references, which are great follow-up’s on the topic:\n\nLovelace, Nowosad, and Muenchow (2019) offer a great introduction.\nChapter 6 of Brunsdon and Comber (2015), in particular subsections 6.3 and 6.7.\nBivand, Pebesma, and Gómez-Rubio (2013) provides an in-depth treatment of spatial data in R.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#dependencies-1",
    "href": "02c_point-pattern.html#dependencies-1",
    "title": "3  Point data analysis",
    "section": "3.7 Dependencies",
    "text": "3.7 Dependencies\nWe will rely on the following libraries in this section, all of them included in Section 0.1:\n\n# data manipulation, transformation and visualisation\nlibrary(tidyverse)\n# spatial data manipulation\nlibrary(sf)\nlibrary(sp)\n# data visualisation\nlibrary(gridExtra)\n# basemap\nlibrary(basemapR)\n# interpolation\nlibrary(gstat)\nlibrary(hexbin)\n\nBefore we start any analysis, let us set the path to the directory where we are working. We can easily do that with setwd(). Please replace in the following line the path to the folder where you have placed this file -and where the house_transactions folder with the data lives.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#data-1",
    "href": "02c_point-pattern.html#data-1",
    "title": "3  Point data analysis",
    "section": "3.8 Data",
    "text": "3.8 Data\nFor this session, we will use the set of Airbnb properties for San Diego (US), borrowed from the “Geographic Data Science with Python” book (see here for more info on the dataset source). This covers the point location of properties advertised on the Airbnb website in the San Diego region.\nLet us start by reading the data, which comes in a GeoJSON:\n\ndb &lt;- st_read(\"data/abb_sd/regression_db.geojson\")\n\nReading layer `regression_db' from data source \n  `/Users/franciscorowe/Library/CloudStorage/Dropbox/Francisco/Research/presentations/2025/montevideo-2/github/smds-pop/data/abb_sd/regression_db.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 6110 features and 19 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2812 ymin: 32.57349 xmax: -116.9553 ymax: 33.08311\nGeodetic CRS:  WGS 84\n\n\nWe can then examine the columns of the table with the colnames method:\n\ncolnames(db)\n\n [1] \"accommodates\"       \"bathrooms\"          \"bedrooms\"          \n [4] \"beds\"               \"neighborhood\"       \"pool\"              \n [7] \"d2balboa\"           \"coastal\"            \"price\"             \n[10] \"log_price\"          \"id\"                 \"pg_Apartment\"      \n[13] \"pg_Condominium\"     \"pg_House\"           \"pg_Other\"          \n[16] \"pg_Townhouse\"       \"rt_Entire_home.apt\" \"rt_Private_room\"   \n[19] \"rt_Shared_room\"     \"geometry\"          \n\n\nThe rest of this session will focus on two main elements of the table: the spatial dimension (as stored in the point coordinates), and the nightly price values, expressed in USD and contained in the price column. To get a sense of what they look like first, let us plot both. We can get a quick look at the non-spatial distribution of house values with the following commands:\n\n# Create the histogram\nqplot( data = db, x = price)\n\n\n\n\n\n\n\n\nThis basically shows there is a lot of values concentrated around the lower end of the distribution but a few very large ones. A usual transformation to shrink these differences is to take logarithms. The original table already contains an additional column with the logarithm of each price (log_price).\n\n# Create the histogram\nqplot( data = db, x = log_price )\n\n\n\n\n\n\n\n\nTo obtain the spatial distribution of these houses, we need to focus on the geometry column. The easiest, quickest (and also “dirtiest”) way to get a sense of what the data look like over space is using plot:\n\nplot(st_geometry(db))\n\n\n\n\n\n\n\n\nNow this has the classic problem of cluttering: some portions of the map have so many points that we can’t tell what the distribution is like. To get around this issue, there are two solutions: binning and smoothing.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#binning-1",
    "href": "02c_point-pattern.html#binning-1",
    "title": "3  Point data analysis",
    "section": "3.9 Binning",
    "text": "3.9 Binning\nThe two-dimensional sister of histograms are binning maps: we divide each of the two dimensions into “buckets”, and count how many points fall within each bucket. Unlike histograms, we encode that count with a color gradient rather than a bar chart over an additional dimension (for that, we would need a 3D plot). These “buckets” can be squares (left) or hexagons (right):\n\n      # Squared binning\n# Set up plot\nsqbin &lt;- ggplot( ) + \n# Add 2D binning with the XY coordinates as\n# a dataframe\n  geom_bin2d(\n    data = as.data.frame( st_coordinates( db ) ), \n    aes( x = X, y = Y)\n  ) + \n  # set theme \n  theme_plot_tufte()\n      # Hex binning\n# Set up plot\nhexbin &lt;- ggplot() +\n# Add hex binning with the XY coordinates as\n# a dataframe \n  geom_hex(\n    data = as.data.frame( st_coordinates( db ) ),\n    aes( x = X, y = Y)\n  ) +\n# Use viridis for color encoding (recommended)\n  scale_fill_continuous( type = \"viridis\" ) +\n  theme_plot_tufte()\n      # Bind in subplots\ngrid.arrange( sqbin, hexbin, ncol = 2 )",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#kde-1",
    "href": "02c_point-pattern.html#kde-1",
    "title": "3  Point data analysis",
    "section": "3.10 KDE",
    "text": "3.10 KDE\nKernel Density Estimation (KDE) is a technique that creates a continuous representation of the distribution of a given variable, such as house prices. Although theoretically it can be applied to any dimension, usually, KDE is applied to either one or two dimensions.\n\n3.10.1 One-dimensional KDE\nKDE over a single dimension is essentially a contiguous version of a histogram. We can see that by overlaying a KDE on top of the histogram of logs that we have created before:\n\n# Create the base\nbase &lt;- ggplot(db, aes(x=log_price))\n# Histogram\nhist &lt;- base + \n  geom_histogram(bins=50, aes(y=..density..))\n# Overlay density plot\nkde &lt;- hist + \n  geom_density(fill=\"#FF6666\", alpha=0.5, colour=\"#FF6666\") +\n  theme_plot_tufte()\nkde\n\n\n\n\n\n\n\n\nThe key idea is that we are smoothing out the discrete binning that the histogram involves. Note how the histogram is exactly the same as above shape-wise, but it has been rescalend on the Y axis to reflect probabilities rather than counts.\n\n\n3.10.2 Two-dimensional (spatial) KDE\nGeography, at the end of the day, is usually represented as a two-dimensional space where we locate objects using a system of dual coordinates, X and Y (or latitude and longitude). Thanks to that, we can use the same technique as above to obtain a smooth representation of the distribution of a two-dimensional variable. The crucial difference is that, instead of obtaining a curve as the output, we will create a surface, where intensity will be represented with a color gradient, rather than with the second dimension, as it is the case in the figure above.\nTo create a spatial KDE in R, we can use general tooling for non-spatial points, such as the stat_density2d_filled method:\n\n# Create the KDE surface\nkde &lt;- ggplot(data = db) +\n  stat_density2d_filled(alpha = 1,\n    data = as.data.frame(st_coordinates(db)), \n    aes(x = X, y = Y),\n    n = 100\n  ) +\n  # Tweak the color gradient\n  scale_color_viridis_c() +\n  # White theme\n  theme_plot_tufte() \nkde\n\n\n\n\n\n\n\n\nThis approach generates a surface that represents the density of dots, that is an estimation of the probability of finding a house transaction at a given coordinate. However, without any further information, they are hard to interpret and link with previous knowledge of the area. To bring such context to the figure, we can plot an underlying basemap, using a cloud provider such as Google Maps or, as in this case, OpenStreetMap. To do it, we will leverage the library basemapR, which is designed to play nicely with the ggplot2 family (hence the seemingly counterintuitive example above). Before we can plot them with the online map, we need to reproject them though.\n\nbbox_db &lt;- st_bbox(db)\nggplot() +\n  base_map(bbox_db, increase_zoom = 2, basemap = \"positron\") +\n  #geom_sf(data = db, fill = NA) +\n  stat_density2d_filled(alpha = 0.7,\n    data = as.data.frame(st_coordinates(db)), \n    aes(x = X, y = Y),\n    n = 100\n  )",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#spatial-interpolation-1",
    "href": "02c_point-pattern.html#spatial-interpolation-1",
    "title": "3  Point data analysis",
    "section": "3.11 Spatial Interpolation",
    "text": "3.11 Spatial Interpolation\nThe previous section demonstrates how to visualize the distribution of a set of spatial objects represented as points. In particular, given a bunch of house locations, it shows how one can effectively visualize their distribution over space and get a sense of the density of occurrences. Such visualization, because it is based on KDE, is based on a smooth continuum, rather than on a discrete approach (as a choropleth would do, for example).\nMany times however, we are not particularly interested in learning about the density of occurrences, but about the distribution of a given value attached to each location. Think for example of weather stations and temperature: the location of the stations is no secret and rarely changes, so it is not of particular interest to visualize the density of stations; what we are usually interested instead is to know how temperature is distributed over space, given we only measure it in a few places. One could argue the example we have been working with so far, house prices in AirBnb, fits into this category as well: although where a house is advertised may be of relevance, more often we are interested in finding out what the “surface of price” looks like. Rather than where are most houses being advertised? we usually want to know where the most expensive or most affordable houses are located.\nIn cases where we are interested in creating a surface of a given value, rather than a simple density surface of occurrences, KDE cannot help us. In these cases, what we are interested in is spatial interpolation, a family of techniques that aim at exactly that: creating continuous surfaces for a particular phenomenon (e.g. temperature, house prices) given only a finite sample of observations. Spatial interpolation is a large field of research that is still being actively developed and that can involve a substantial amount of mathematical complexity in order to obtain the most accurate estimates possible7. In this chapter, we will introduce the simplest possible way of interpolating values, hoping this will give you a general understanding of the methodology and, if you are interested, you can check out further literature. For example, Banerjee, Carlin, and Gelfand (2014) or Cressie (2015) are hard but good overviews.\n\n3.11.1 Inverse Distance Weight (IDW) interpolation\nThe technique we will cover here is called Inverse Distance Weighting, or IDW for convenience. Brunsdon and Comber (2015) offer a good description:\n\nIn the inverse distance weighting (IDW) approach to interpolation, to estimate the value of \\(z\\) at location \\(x\\) a weighted mean of nearby observations is taken […]. To accommodate the idea that observations of \\(z\\) at points closer to \\(x\\) should be given more importance in the interpolation, greater weight is given to these points […]\n— Page 204\n\nThe math8 is not particularly complicated and may be found in detail elsewhere (the reference above is a good starting point), so we will not spend too much time on it. More relevant in this context is the intuition behind. The idea is that we will create a surface of house price by smoothing many values arranged along a regular grid and obtained by interpolating from the known locations to the regular grid locations. This will give us full and equal coverage to soundly perform the smoothing.\nEnough chat, let’s code9.\nFrom what we have just mentioned, there are a few steps to perform an IDW spatial interpolation:\n\nCreate a regular grid over the area where we have house transactions.\nObtain IDW estimates for each point in the grid, based on the values of \\(k\\) nearest neighbors.\nPlot a smoothed version of the grid, effectively representing the surface of house prices.\n\nLet us go in detail into each of them10. First, let us set up a grid for the extent of the bounding box of our data (not the use of pipe, %&gt;%, operator to chain functions):\n\nsd.grid &lt;- db %&gt;%\n  st_bbox() %&gt;%\n  st_as_sfc() %&gt;%\n  st_make_grid(\n    n = 100,\n    what = \"centers\"\n  ) %&gt;%\n  st_as_sf() %&gt;%\n  cbind(., st_coordinates(.))\n\nThe object sd.grid is a regular grid with 10,000 (\\(100 \\times 100\\)) equally spaced cells:\n\nsd.grid\n\nSimple feature collection with 10000 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2795 ymin: 32.57604 xmax: -116.9569 ymax: 33.08056\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           X        Y                          x\n1  -117.2795 32.57604 POINT (-117.2795 32.57604)\n2  -117.2763 32.57604 POINT (-117.2763 32.57604)\n3  -117.2730 32.57604  POINT (-117.273 32.57604)\n4  -117.2698 32.57604 POINT (-117.2698 32.57604)\n5  -117.2665 32.57604 POINT (-117.2665 32.57604)\n6  -117.2632 32.57604 POINT (-117.2632 32.57604)\n7  -117.2600 32.57604   POINT (-117.26 32.57604)\n8  -117.2567 32.57604 POINT (-117.2567 32.57604)\n9  -117.2535 32.57604 POINT (-117.2535 32.57604)\n10 -117.2502 32.57604 POINT (-117.2502 32.57604)\n\n\nNow, sd.grid only contain the location of points to which we wish to interpolate. That is, we now have our “target” geography for which we’d like to have AirBnb prices, but we don’t have price estimates. For that, on to the IDW, which will generate estimates for locations in sd.grid based on the observed prices in db. Again, this is hugely simplified by gstat:\n\nidw.hp &lt;- idw(\n  price ~ 1,         # Formula for IDW\n  locations = db,    # Initial locations with values\n  newdata=sd.grid,   # Locations we want predictions for\n  nmax = 150         # Limit the number of neighbours for IDW\n)\n\n[inverse distance weighted interpolation]\n\n\nBoom! We’ve got it. Let us pause for a second to see how we just did it. First, we pass price ~ 1. This specifies the formula we are using to model house prices. The name on the left of ~ represents the variable we want to explain, while everything to its right captures the explanatory variables. Since we are considering the simplest possible case, we do not have further variables to add, so we simply write 1. Then we specify the original locations for which we do have house prices (our original db object), and the points where we want to interpolate the house prices (the sd.grid object we just created above). One more note: by default, idw uses all the available observations, weighted by distance, to provide an estimate for a given point. If you want to modify that and restrict the maximum number of neighbors to consider, you need to tweak the argument nmax, as we do above by using the 150 nearest observations to each point11.\nThe object we get from idw is another spatial table, just as db, containing the interpolated values. As such, we can inspect it just as with any other of its kind. For example, to check out the top of the estimated table:\n\nhead(idw.hp)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.2795 ymin: 32.57604 xmax: -117.2632 ymax: 32.57604\nGeodetic CRS:  WGS 84\n  var1.pred var1.var                   geometry\n1  295.6100       NA POINT (-117.2795 32.57604)\n2  295.1651       NA POINT (-117.2763 32.57604)\n3  296.5927       NA  POINT (-117.273 32.57604)\n4  288.2252       NA POINT (-117.2698 32.57604)\n5  281.5522       NA POINT (-117.2665 32.57604)\n6  268.3567       NA POINT (-117.2632 32.57604)\n\n\nThe column we will pay attention to is var1.pred. For a hypothetical house advertised at the location in the first row of point in sd.grid, the price IDW would guess it would cost, based on prices nearby, is the first element of column var1.pred in idw.hp.\n\n\n3.11.2 A surface of housing prices\nOnce we have the IDW object computed, we can plot it to explore the distribution, not of AirBnb locations in this case, but of house prices over the geography of San Diego. To do this using ggplot2, we first append the coordinates of each grid cell as columns of the table:\n\nidw.hp = idw.hp %&gt;%\n  cbind(st_coordinates(.))\n\nNow, we can visualise the surface using standard ggplot2 tools:\n\nggplot(idw.hp, aes(x = X, y = Y, fill = var1.pred)) +\n  geom_raster()\n\n\n\n\n\n\n\n\nAnd we can “dress it up” a bit further:\n\nggplot(idw.hp, aes(x = X, y = Y, fill = var1.pred)) +\n  geom_raster() +\n  scale_fill_viridis_b() +\n  theme_void() +\n  geom_sf(alpha=0)\n\n\n\n\n\n\n\n\nLooking at this, we can start to tell some patterns. To bring in context, it would be great to be able to add a basemap layer, as we did for the KDE. This is conceptually very similar to what we did above, starting by reprojecting the points and continuing by overlaying them on top of the basemap. However, technically speaking it is not possible because ggmap –the library we have been using to display tiles from cloud providers– does not play well with our own rasters (i.e. the price surface). At the moment, it is surprisingly tricky to get this to work, so we will park it for now12.\n\n\n3.11.3 “What should the next house’s price be?”\nThe last bit we will explore in this session relates to prediction for new values. Imagine you are a real state data scientist working for Airbnb and your boss asks you to give an estimate of how much a new house going into the market should cost. The only information you have to make such a guess is the location of the house. In this case, the IDW model we have just fitted can help you. The trick is realizing that, instead of creating an entire grid, all we need is to obtain an estimate of a single location.\nLet us say, a new house is going to be advertised on the coordinates X = -117.02259063720702, Y = 32.76511965117273 as expressed in longitude and latitude. In that case, we can do as follows:\n\npt &lt;- c(X = -117.02259063720702, Y = 32.76511965117273) %&gt;%\n  st_point() %&gt;%\n  st_sfc() %&gt;%\n  st_sf(crs = \"EPSG:4326\") %&gt;%\n  st_transform(st_crs(db))\nidw.one &lt;- idw(price ~ 1, locations=db, newdata=pt)\n\n[inverse distance weighted interpolation]\n\nidw.one\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -117.0226 ymin: 32.76512 xmax: -117.0226 ymax: 32.76512\nGeodetic CRS:  WGS 84\n  var1.pred var1.var                   geometry\n1  171.4141       NA POINT (-117.0226 32.76512)\n\n\nAnd, as show above, the estimated value is $171.414133413.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#questions-1",
    "href": "02c_point-pattern.html#questions-1",
    "title": "3  Point data analysis",
    "section": "3.12 Questions",
    "text": "3.12 Questions\nWe will be using the Madrid AirBnb dataset:\n\nmad_abb &lt;- st_read(\"data/assignment_1_madrid/madrid_abb.gpkg\")\n\nReading layer `madrid_abb' from data source \n  `/Users/franciscorowe/Library/CloudStorage/Dropbox/Francisco/Research/presentations/2025/montevideo-2/github/smds-pop/data/assignment_1_madrid/madrid_abb.gpkg' \n  using driver `GPKG'\nSimple feature collection with 18399 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -3.86391 ymin: 40.33243 xmax: -3.556 ymax: 40.56274\nGeodetic CRS:  WGS 84\n\n\nThis is fairly similar in spirit to the one from San Diego we have relied on for the chapter, although the column set is not exactly the same:\n\ncolnames(mad_abb)\n\n [1] \"price\"           \"price_usd\"       \"log1p_price_usd\" \"accommodates\"   \n [5] \"bathrooms_text\"  \"bathrooms\"       \"bedrooms\"        \"beds\"           \n [9] \"neighbourhood\"   \"room_type\"       \"property_type\"   \"WiFi\"           \n[13] \"Coffee\"          \"Gym\"             \"Parking\"         \"km_to_retiro\"   \n[17] \"geom\"           \n\n\nFor this set of questions, the only two columns we will need is geom, which contains the point geometries, and price_usd, which record the price of the AirBnb property in USD.\nWith this at hand, answer the following questions:\n\nCreate a KDE that represents the density of locations of AirBnb properties in Madrid\nUsing inverse distance weighting, create a surface of AirBnb prices\n\n\n\n\n\nBanerjee, Sudipto, Bradley P Carlin, and Alan E Gelfand. 2014. Hierarchical Modeling and Analysis for Spatial Data. Crc Press.\n\n\nBivand, Roger S., Edzer Pebesma, and Virgilio Gómez-Rubio. 2013. Applied Spatial Data Analysis with r. Springer New York. https://doi.org/10.1007/978-1-4614-7618-4.\n\n\nBrunsdon, Chris, and Lex Comber. 2015. An Introduction to r for Spatial Analysis & Mapping. Sage.\n\n\nCressie, Noel. 2015. Statistics for Spatial Data. John Wiley & Sons.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. Chapman; Hall/CRC. https://doi.org/10.1201/9780203730058.",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  },
  {
    "objectID": "02c_point-pattern.html#footnotes",
    "href": "02c_point-pattern.html#footnotes",
    "title": "3  Point data analysis",
    "section": "",
    "text": "There is also an important economic incentive to do this: some of the most popular applications are in the oil and gas or mining industries. In fact, the very creator of this technique, Danie G. Krige, was a mining engineer. His name is usually used to nickname spatial interpolation as kriging.↩︎\nEssentially, for any point \\(x\\) in space, the IDW estimate for value \\(z\\) is equivalent to \\(\\hat{z} (x) = \\dfrac{\\sum_i w_i z_i}{\\sum_i w_i}\\) where \\(i\\) are the observations for which we do have a value, and \\(w_i\\) is a weight given to location \\(i\\) based on its distance to \\(x\\).↩︎\nIf you want a complementary view of point interpolation in R, you can read more on this fantastic blog post↩︎\nHave a play with this because the results do change significantly. Can you reason why?↩︎\nBONUS if you can figure out a way to do it yourself!↩︎\nPRO QUESTION Is that house expensive or cheap, as compared to the other houses sold in this dataset? Can you figure out where the house is in the distribution?↩︎\nThere is also an important economic incentive to do this: some of the most popular applications are in the oil and gas or mining industries. In fact, the very creator of this technique, Danie G. Krige, was a mining engineer. His name is usually used to nickname spatial interpolation as kriging.↩︎\nEssentially, for any point \\(x\\) in space, the IDW estimate for value \\(z\\) is equivalent to \\(\\hat{z} (x) = \\dfrac{\\sum_i w_i z_i}{\\sum_i w_i}\\) where \\(i\\) are the observations for which we do have a value, and \\(w_i\\) is a weight given to location \\(i\\) based on its distance to \\(x\\).↩︎\nIf you want a complementary view of point interpolation in R, you can read more on this fantastic blog post↩︎\nFor the relevant calculations, we will be using the gstat library.↩︎\nHave a play with this because the results do change significantly. Can you reason why?↩︎\nBONUS if you can figure out a way to do it yourself!↩︎\nPRO QUESTION Is that house expensive or cheap, as compared to the other houses sold in this dataset? Can you figure out where the house is in the distribution?↩︎",
    "crumbs": [
      "Content",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Point data analysis</span>"
    ]
  }
]